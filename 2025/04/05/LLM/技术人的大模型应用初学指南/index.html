<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>技术人的大模型应用初学指南 | 易锦风的博客</title><meta name="author" content="formeasy"><meta name="copyright" content="formeasy"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="技术人的大模型应用初学指南"><meta property="og:type" content="article"><meta property="og:title" content="技术人的大模型应用初学指南"><meta property="og:url" content="http://www.formeasy.cc/2025/04/05/LLM/%E6%8A%80%E6%9C%AF%E4%BA%BA%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%88%9D%E5%AD%A6%E6%8C%87%E5%8D%97/index.html"><meta property="og:site_name" content="易锦风的博客"><meta property="og:description" content="技术人的大模型应用初学指南"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://www.formeasy.cc/2025/04/05/LLM/%E6%8A%80%E6%9C%AF%E4%BA%BA%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%88%9D%E5%AD%A6%E6%8C%87%E5%8D%97/e74ce26e-86d9-4380-881b-c7d75ed44e12.png"><meta property="article:published_time" content="2025-04-05T13:10:24.000Z"><meta property="article:modified_time" content="2025-04-25T01:58:12.367Z"><meta property="article:author" content="formeasy"><meta property="article:tag" content="LLM"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://www.formeasy.cc/2025/04/05/LLM/%E6%8A%80%E6%9C%AF%E4%BA%BA%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%88%9D%E5%AD%A6%E6%8C%87%E5%8D%97/e74ce26e-86d9-4380-881b-c7d75ed44e12.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://www.formeasy.cc/2025/04/05/LLM/%E6%8A%80%E6%9C%AF%E4%BA%BA%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%88%9D%E5%AD%A6%E6%8C%87%E5%8D%97/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>(()=>{var e={set:(e,t,a)=>{a&&(a=Date.now()+864e5*a,localStorage.setItem(e,JSON.stringify({value:t,expiry:a})))},get:e=>{var t=localStorage.getItem(e);if(t){var{value:t,expiry:a}=JSON.parse(t);if(!(Date.now()>a))return t;localStorage.removeItem(e)}}},t=(window.btf={saveToLocal:e,getScript:(o,n={})=>new Promise((e,t)=>{const a=document.createElement("script");a.src=o,a.async=!0,Object.entries(n).forEach(([e,t])=>a.setAttribute(e,t)),a.onload=a.onreadystatechange=()=>{a.readyState&&!/loaded|complete/.test(a.readyState)||e()},a.onerror=t,document.head.appendChild(a)}),getCSS:(o,n)=>new Promise((e,t)=>{const a=document.createElement("link");a.rel="stylesheet",a.href=o,n&&(a.id=n),a.onload=a.onreadystatechange=()=>{a.readyState&&!/loaded|complete/.test(a.readyState)||e()},a.onerror=t,document.head.appendChild(a)}),addGlobalFn:(e,t,a=!1,o=window)=>{var n;e.startsWith("pjax")||((n=o.globalFn||{})[e]=n[e]||{},n[e][a||Object.keys(n[e]).length]=t,o.globalFn=n)}},()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")}),a=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")},o=(btf.activateDarkMode=t,btf.activateLightMode=a,e.get("theme")),t=("dark"===o?t():"light"===o&&a(),e.get("aside-status"));void 0!==t&&document.documentElement.classList.toggle("hide-aside","hide"===t);/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!1,languages:{hits_empty:"未找到符合您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1,highlightFullpage:!1,highlightMacStyle:!0},copy:{success:"复制成功",error:"复制失败",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"技术人的大模型应用初学指南",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,isShuoshuo:!1}</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="易锦风的博客" type="application/atom+xml"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/butterfly-icon.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">213</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">38</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我们</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(/2025/04/05/LLM/技术人的大模型应用初学指南/e74ce26e-86d9-4380-881b-c7d75ed44e12.png)"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/icon.png" alt="Logo"><span class="site-name">易锦风的博客</span></a><a class="nav-page-title" href="/"><span class="site-name">技术人的大模型应用初学指南</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我们</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">技术人的大模型应用初学指南</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-04-05T13:10:24.000Z" title="发表于 2025-04-05 21:10:24">2025-04-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-04-25T01:58:12.367Z" title="更新于 2025-04-25 09:58:12">2025-04-25</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/">软件工程</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">10.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>30分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><blockquote><h2 id="excerpt"><a class="markdownIt-Anchor" href="#excerpt"></a> Excerpt</h2><p>随着人工智能技术的快速发展，检索增强生成（RAG）作为一种结合检索与生成的创新技术，正在重新定义信息检索的方式。本文深入探讨了 RAG 的核心原理及其在实际应用中的挑战与解决方案。文章首先分析了通用大模型在知识局限性、幻觉问题和数据安全性等方面的不足，随后详细介绍了 RAG 通过 “检索 + 生成” 模式如何有效解决这些问题。具体而言，RAG 利用向量数据库高效存储与检索目标知识，并结合大模型生成合理答案。此外，文章还对 RAG 的关键技术进行了全面解析，包括文本清洗、文本切块、向量嵌入、召回优化及提示词工程等环节。最后，针对 RAG 系统的召回效果与模型回答质量，本文提出了多种评估方法，为实际开发提供了重要参考。通过本文，读者可以全面了解 RAG 技术的原理、实现路径及其在信息检索领域的革命性意义。</p></blockquote><hr><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="182c7a72-46e5-441f-8edf-a610ad4963be.png" alt=""></p><p><strong>前言</strong></p><p>人工智能（AI）时代的到来为技术人员提供了丰富的学习和发展机会。对于没有算法背景的技术同学来说，迎接这种新兴机遇与挑战并做好应对准备和知识储备是非常重要的。</p><p>结合笔者这一段对于大模型和 AI 技术的一些学习以及对基于 AI 改造的诸多实际应用场景的了解。于是就写了这篇文章。另外，本篇文章不会用过多的篇幅来讲算法基础的内容，而把重点放在 AI 应用的核心技术概念的理解上。</p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="576ad66d-81dd-45a0-9167-3f598cd60006.png" alt=""></p><p><strong>人工智能术语概述</strong></p><p>想必大家在刚开始阅读人工智能相关的文章或书籍的时候，总是听到诸如 LLM，chatGPT，RAG，Agent 等等的术语，但是不知道这些术语对应的技术点关联性在哪里，没关系，咱们首先来学习下这些术语的定义：</p><ul><li><p>AI：Artificial Intelligence 的缩写，指 “人工智能”，人工智能是指模拟人类智能的计算机系统或软件，使其能够执行诸如学习、推理、问题解决、感知、语言理解等复杂任务。</p></li><li><p>生成式 AI：是一种人工智能技术，能够自动生成新的内容，如文本、图像、音频和视频等。与传统的 AI 不同，生成式 AI 不仅能分析和理解数据，还能基于其学习到的信息创造出新的内容。</p></li><li><p>AIGC：AI Generated Content 的缩写，意指由人工智能生成的内容。在算法和数码内容制作领域，AIGC 涉及使用人工智能技术生成各种形式的内容，比如文字、图像、视频、音乐等。</p></li><li><p>NLP：Natural Language Processing 的缩写，指 “自然语言处理”，自然语言处理是人工智能的一个子领域，主要研究计算机如何理解、解释和生成人类语言。NLP 技术包括文本分析、语言生成、机器翻译、情感分析、对话系统等。</p></li><li><p>Transformer：一种用于自然语言处理（NLP）任务的深度学习模型，最初由 Vaswani 等人在 2017 年的论文中提出。它引入了一种名为 “自注意力”（self-attention）的机制，能够有效地处理序列数据，且在许多 NLP 任务，如机器翻译、文本生成和语言建模中取得了巨大的成功。</p></li><li><p>BERT：Bidirectional Encoder Representations from Transformers 的缩写，是一种自然语言处理（NLP）的预训练模型。它由 Google AI 研究团队于 2018 年首次提出。BERT 的主要创新在于它使用了双向（即上下文敏感）的 Transformer 模型来对文本进行编码。</p></li><li><p>PEFT：Parameter-Efficient Fine-Tuning 的缩写，中文高效参数微调，这是一种微调机器学习模型的方法，旨在减少需要更新的参数数量，从而降低计算成本和存储需求，同时保持模型性能。PEFT 技术在大型预训练模型（如 BERT、GPT 等）的下游任务适配中尤为重要，因为直接微调这些模型可能会耗费大量计算资源和时间。</p></li><li><p>LoRA：Low-Rank Adaptation 的缩写，一种用于微调大规模语言模型的一种技术。它通过将模型的权重分解成低秩矩阵来显著减少参数数量和计算开销，从而使得模型在资源受限的环境中也能进行高效的适应性调整。</p></li><li><p>LLM：Large Language Model 的缩写，指 “大语言模型”，这类模型是基于机器学习和深度学习技术，特别是自然语言处理（NLP）中的一种技术。大语言模型通过大量的文本数据进行训练，以生成、理解和处理自然语言。一些著名的 LLM 示例包括 OpenAI 的 GPT（Generative Pre-trained Transformer）系列模型，如 GPT-3 和 GPT-4</p></li><li><p>RAG：Retrieval-Augmented Generation 的缩写，指 “检索增强生成”，这是一个跨越检索和生成任务的框架，通过先从数据库或文档集合中检索到相关信息，然后利用生成模型（如 Transformer 模型）来生成最终的输出。目前在技术发展趋势和应用落地上，RAG 是工程同学较为值得探索的领域。</p></li><li><p>Agent：中文叫智能体，一个能独立执行任务和做出决策的实体，在人工智能中，Agent 可以是一个机器人，一个虚拟助手，或是一个智能软件系统，它能够通过学习和推理来完成复杂任务。在多 Agent 系统中，多个独立的 Agents 相互协作或竞争，以共同解决问题或完成任务。</p></li><li><p>GPT：Generative Pre-trained Transformer 的缩写，指 “生成式预训练变换器”，GPT 模型利用大量文本数据进行预训练，然后可以通过微调来执行特定任务，例如语言生成、回答问题、翻译、文本摘要等。</p></li><li><p>LLaMA：Large Language Model Meta AI 的缩写，是由 Meta 开发的一系列大型自然语言处理模型。这些模型在处理文本生成和理解任务方面表现出色，类似于其他著名的大型语言模型如 GPT-3</p></li><li><p>chatGPT：由 OpenAI 开发的一种基于 GPT（生成预训练变换模型）架构的人工智能聊天机器人。它使用自然语言处理技术，能够理解并生成类似人类的文本回复。可以看做是一种 Agent。</p></li><li><p>Prompt：指的是提供给模型的一段初始文本，用于引导模型生成后续的内容。</p></li><li><p>Embedding：中文叫嵌入，是一种将高维数据映射到低维空间的技术，但仍尽可能保留原数据的特征和结构。嵌入技术通常用于处理和表示复杂的数据如文本、图像、音乐以及其他高维度的数据类型。</p></li></ul><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="2a470d67-9085-4f14-b186-66b91781d344.png" alt=""></p><p><strong>大模型应用发展趋势</strong></p><h2 id="向量数据库"><a class="markdownIt-Anchor" href="#向量数据库"></a> <strong>▐  向量数据库</strong></h2><p>随着互联网内容化的飞速发展，以音视频等多媒体内容为代表的非结构化数据呈现出高速增长的趋势。图片、音频、视频等非结构化数据的存储和检索需求也变得越来越多。</p><p>IDC DataSphere 数据显示，到 2027 年全球非结构化数据将占到数据总量的 86.8%，达到 246.9ZB；全球数据总量从 103.67ZB 增长至 284.30ZB，CAGR 为 22.4%，呈现稳定增长态势。</p><p>Link：<a target="_blank" rel="noopener" href="https://www.idc.com/getdoc.jsp?containerId=prCHC51814824">https://www.idc.com/getdoc.jsp?containerId=prCHC51814824</a></p><p>通常，为了更有效地管理非结构化数据，常见的做法是将其转换为向量表示，并存储在向量数据库中。这种转换过程通常被称为向量化或嵌入（Embedding）。通过将文本、图像或其他非结构化数据映射到高维向量空间，我们可以捕捉数据的语义特征和潜在关系。向量数据库通过在「向量表示」上构建索引，实现快速的相似性搜索。</p><p>向量数据库是用于存储和查询高维向量数据的数据库，通常在搜索、推荐系统、图像识别、自然语言处理等领域中广泛使用。随着 AI 创新应用的不断涌现，对于向量数据库需求也大增。</p><p><strong><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="e74ce26e-86d9-4380-881b-c7d75ed44e12.png" alt=""></strong></p><p>下面是一些常用的向量数据库。</p><p>1. Faiss (Facebook AI Similarity Search)：</p><ul><li><p>开发者：Facebook AI Research</p></li><li><p>特点：高效的相似性搜索和密集向量聚类，支持 CPU 和 GPU 加速。</p></li><li><p>适用场景：图像相似性搜索、大规模推荐系统等。</p></li></ul><p>2. Annoy (Approximate Nearest Neighbors Oh Yeah)：</p><ul><li><p>开发者：Spotify</p></li><li><p>特点：基于内存的高效最近邻搜索，使用构建的可持久化树数据结构。</p></li><li><p>适用场景：音乐推荐、快速搜索等。</p></li></ul><p>3. HNSW (Hierarchical Navigable Small World)：</p><ul><li><p>开发者：Yury Malkov（和其他社区贡献者）</p></li><li><p>特点：小世界图算法，高效的近似最近邻搜索，支持动态插入和删除。</p></li><li><p>适用场景：实时搜索和推荐系统。</p></li></ul><p>4. Elasticsearch with k-NN Plugin：</p><ul><li><p>开发者：Elastic</p></li><li><p>特点：在 Elasticsearch 之上添加 k-NN 搜索功能，结合全文搜索和向量搜索。</p></li><li><p>适用场景：综合搜索引擎，需要同时支持文本和向量查询的场景。</p></li></ul><p>5. Milvus：</p><ul><li><p>开发者：ZILLIZ</p></li><li><p>特点：分布式、高性能向量数据库，支持大规模数据管理和检索。</p></li><li><p>适用场景：图像、视频、文本等大规模向量数据的存储和检索。</p></li></ul><p>6. Pinecone：</p><ul><li><p>开发者：Pinecone</p></li><li><p>特点：专用于机器学习应用程序的向量数据库，易于集成和扩展。</p></li><li><p>适用场景：个性化推荐、语义搜索、实时机器学习应用等。</p></li></ul><p>7. Weaviate：</p><ul><li><p>开发者：SeMI Technologies</p></li><li><p>特点：开源的向量搜索引擎，支持上下文感知的语义搜索，扩展性强。</p></li><li><p>适用场景：知识图谱构建、语义搜索、推荐系统。</p></li></ul><p>8. Vectara：</p><ul><li><p>开发者：Vectara, Inc.</p></li><li><p>特点：基于向量的全托管搜索服务，专注于语义搜索和相关性。</p></li><li><p>适用场景：搜索引擎优化、自然语言处理应用。</p></li></ul><p>上述所提到的目前主流的向量数据库方案，在向量数据的存储成本、召回率等方面都面临较大的挑战。随着非结构化数据的进一步增长，成本和召回率的挑战会变得越来越棘手。在向量数据库的演讲方向上目前有以下发展趋势</p><h4 id="1-存储和索引优化"><a class="markdownIt-Anchor" href="#1-存储和索引优化"></a> 1. 存储和索引优化</h4><ul><li><p>量化技术：使用向量量化（Vector Quantization, VQ）技术，例如产品量化（Product Quantization, PQ）或乘积量化（Additive Quantization, AQ），可以在保证精度的同时大幅度减少存储和计算资源。</p></li><li><p>压缩向量：采用哈希方法如局部敏感哈希（Locality-Sensitive Hashing, LSH）来减少存储消耗，并加速相似性搜索。</p></li><li><p>分布式存储：使用分布式文件系统和数据库（如 Apache Hadoop、Cassandra）可以优化存储和查询的大规模向量数据。</p></li><li><p>存储器级别调整：利用固态硬盘（SSD）甚至是新兴的持久化内存（Persistent Memory, PMEM）来在内存和磁盘之间找到平衡，优化存储成本。</p></li></ul><h4 id="2-召回率优化"><a class="markdownIt-Anchor" href="#2-召回率优化"></a> 2. 召回率优化</h4><ul><li><p>混合搜索技术：结合粗粒度和细粒度的索引，例如先使用粗滤技术快速缩小搜索范围，然后进行精确查找。</p></li><li><p>近似最近邻查找（ANN）算法：如 HNSW（Hierarchical Navigable Small World）图、FAISS 中使用的 ANN 算法可以在保证高召回率的基础上优化搜索速度。</p></li><li><p>多层次检索：分层结构的检索方法，从粗到细进行，逐步提高召回率和精度。</p></li></ul><h4 id="3-系统架构和基础设施"><a class="markdownIt-Anchor" href="#3-系统架构和基础设施"></a> 3. 系统架构和基础设施</h4><ul><li><p>云计算和弹性扩展：利用云计算平台（如 AWS、Azure、GCP），按需扩展计算和存储资源，并且利用云端的分布式存储和计算技术来管理大规模数据。</p></li><li><p>边缘计算：部分预处理和向量化工作放到边缘设备进行，减少中心服务器的负担。</p></li></ul><h4 id="4-专用硬件加速"><a class="markdownIt-Anchor" href="#4-专用硬件加速"></a> 4. 专用硬件加速</h4><ul><li><p>GPU 和 TPU：使用专门的硬件加速器，如 GPU（图形处理单元）或 TPU（张量处理单元），以加速向量计算和相似性搜索。</p></li><li><p>FPGA：使用可编程门阵列（FPGA）为特定向量计算任务定制硬件加速，以提高效率和降低延迟。</p></li></ul><h4 id="5-持续优化和更新模型"><a class="markdownIt-Anchor" href="#5-持续优化和更新模型"></a> 5. 持续优化和更新模型</h4><ul><li><p>动态索引更新：随着非结构化数据的增长和变化，保持索引和向量表示的及时性，使用在线或增量更新的方法管理索引。</p></li><li><p>自适应模型：利用机器学习和深度学习模型不断优化向量表示的嵌入质量，使得向量检索更加精准有效。</p></li></ul><h4 id="6-先进的嵌入技术"><a class="markdownIt-Anchor" href="#6-先进的嵌入技术"></a> 6. 先进的嵌入技术</h4><ul><li><p>预训练模型：使用当前的预训练语言模型（如 BERT、GPT-3）进行上下文嵌入，捕捉复杂的语义信息。</p></li><li><p>多模态嵌入：对于不同类型的数据（如文本、图像、视频），使用多模态嵌入模型来统一表示和处理，提升检索性能。</p></li></ul><h2 id="multi-agent"><a class="markdownIt-Anchor" href="#multi-agent"></a> <strong>▐  Multi-Agent</strong></h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="14c7fb6d-6921-41f8-91ea-219cae820eaf.png" alt=""></p><p>在软件领域，“分而治之” 是一种常用的设计和开发理念，而在大模型场景中也同样适用。面向复杂任务场景，多 Agent 方法会将复杂任务分解为子任务，让不同的智能体完成不同的子任务，即专业 “人” 做专业 “事”。因为，拆解任务有助于降低单个大模型的输入复杂度以及理解难度，从而有利于大模型专注于 “做” 一件事情，其性能可能会更好。</p><p>多 Agent 框架的核心交互流程可以概括如下：</p><ul><li><p>controller 更新当前环境的状态，选择下一时刻行动的 agent X</p></li><li><p>agent X 与环境交互，更新自身 memory 信息</p></li><li><p>agent X 调用 LLM，基于指令执行动作，获取输出 message</p></li><li><p>将输出 message 更新到公共环境中</p></li></ul><p>目前在多 Agent 协作方面，目前比较有名的是 AutoGen 框架和 MetaGPT 框架。</p><ul><li>AutoGen 框架</li></ul><p>AutoGen 是一个能让多个 Agent 进行沟通协作的 Python 开源框架。核心解决两个问题：</p><p>第一个问题：如何设计用户可定制、可重用的、能够互相协作的 Agent。AutoGen 是要设计为一个通用的能够适用多种场景的框架，在 AutoGen 的官网 Examples 中给出了在多种场景下能够解决问题的例子，此外在 git 仓库中的 notbook 目录中有 50 + 例子。有解决数学问题场景、有通过开发代码进行分析的场景（比如上一章节的列子）、还有通过五六个 Agent 讨论分析开放问题的场景。所以 Agent 的扩展能力是需要重要考虑问题，AutoGen 中通过支持多种外部工具、不同 LLM、支持 human in the loop 的方式，Agent 之间能够通信的方式来解决扩展问题。</p><p>第二个问题：如何让 Agent 能灵活支持不同模式的会话方式。不同的场景，根据复杂度、问题的类型需要不同的 Agent 会话模式。这里的 “模式” 包括了单轮对话 or 多轮对话、自动选择每轮的 speaker or 通过规则选择、通过自然语言控制逻辑 or 通过代码控制，此外设计需要考虑多个 Agent 之间如何灵活 “组网”，比如三人一组，每组一个 leader，组内互相通信，leader 能够通信的方式。</p><p>为了解决这两个问题，AutoGen 抽象了一些基础概念。</p><h4 id="conversable-agents"><a class="markdownIt-Anchor" href="#conversable-agents"></a> Conversable Agents</h4><p>旨在用于在复杂任务中进行多轮交互。这些智能体能够理解和处理用户输入，维护上下文，并生成合适的响应。Conversable Agents 通常集成了自然语言处理技术，包括自然语言理解（NLU）和自然语言生成（NLG），以提高对话的流畅性和智能性。</p><h4 id="conversation-programming"><a class="markdownIt-Anchor" href="#conversation-programming"></a> Conversation Programming</h4><p>旨在通过自然语言与人工智能系统进行交互，来实现编程和任务自动化。这个概念试图简化编程过程，使得用户无需深厚的编程背景也能使用自然语言描述需求，进而生成可执行的代码或自动化脚本。</p><p>在 Conversation Programming 中，用户通过与人工智能助手进行对话，将具体任务、算法逻辑或问题描述出来，AI 系统则负责理解这些意图并生成相应的代码。这种方式降低了编程的门槛，同时也加速了从想法到实现的过程。</p><ul><li><h4 id="metagpt-框架"><a class="markdownIt-Anchor" href="#metagpt-框架"></a> MetaGPT 框架</h4></li></ul><p>MetaGPT 是一个基于多智能体的元编程框架，它通过将不同的角色（如产品经理、架构师、项目经理等）分配给不同的大型语言模型（LLM），实现软件开发流程的自动化。这个框架特别适合于复杂的编程任务，能够自动生成用户故事、需求分析、数据结构、API 和文档等输出。MetaGPT 使用标准操作程序（SOPs）来指导智能体的协作，旨在提高代码生成的质量和效率 。</p><h4 id="工作流程"><a class="markdownIt-Anchor" href="#工作流程"></a> 工作流程</h4><p>MetaGPT 的主要工作流程和特点包括：</p><ul><li><p>角色定义（Role Definitions）：MetaGPT 通过定义不同的角色（如产品经理、架构师、项目经理等）来封装每个角色的特定技能和业务流程。这些角色类继承自一个基础角色类，具有名称、简介、目标、约束和描述等关键属性。角色定义帮助 LLM 生成符合特定角色要求的行为。</p></li><li><p>任务分解（Task Decomposition）：MetaGPT 将复杂的软件开发任务分解成更小、更易于管理的部分，然后将这些子任务分配给合适的智能体执行。</p></li><li><p>流程标准化（Process Standardization）：MetaGPT 定义了一系列标准化操作，每个操作都具有前缀、LLM 代理、标准化输出模式、执行内容、重试机制等属性。这些标准化操作确保了智能体之间的协作是一致的，输出的结果也是结构化的。</p></li><li><p>知识共享（Knowledge Sharing）：MetaGPT 通过环境日志复制消息，智能体可以根据自己的角色订阅感兴趣的消息类型。这种方式使智能体可以主动获取相关信息，而不是被动地通过对话获取。</p></li><li><p>端到端开发（End-to-End Development）：从产品需求到技术设计，再到具体编码，MetaGPT 通过多智能体的协作可以完成整个软件开发生命周期。</p></li></ul><h4 id="设计层次"><a class="markdownIt-Anchor" href="#设计层次"></a> 设计层次</h4><p>MetaGPT 的设计分为两个主要层次：</p><ul><li><p>Foundational Components Layer（基础组件层）：</p></li><li><p>作用：建立了智能体操作和整个系统范围内信息交流的核心基础构件。这包括了环境（Environment）、记忆（Memory）、角色（Roles）、动作（Actions）和工具（Tools）等元素。</p></li><li><p>功能：</p></li><li><p>Environment：提供了共享的工作空间和通讯功能。</p></li><li><p>Memory：用于存储和检索历史消息。</p></li><li><p>Roles：封装了领域特定的技能和工作流程。</p></li><li><p>Actions：执行模块化的子任务。</p></li><li><p>Tools：提供常用服务和工具。</p></li><li><p>Collaboration Layer（协作层）：</p></li><li><p>作用：在基础组件层之上，协调各个智能体共同解决复杂问题。它建立了合作的基本机制，包括知识共享和封装工作流程。</p></li><li><p>功能：</p></li><li><p>Knowledge Sharing（知识共享）：允许智能体有效地交换信息，贡献到共享的知识库中，从而提高协调能力，减少冗余通讯，提高整体操作效率。</p></li><li><p>Encapsulating Workflows（封装工作流程）：利用 SOP 将复杂任务分解成小而可管理的组件，将这些子任务分配给合适的智能体，并通过标准化的输出来监督其性能，确保其行动符合总体目标。</p></li></ul><p>这两个层次共同构建了 MetaGPT 的框架，为智能体提供了强大的功能，使其能够协作解决复杂任务。</p><h2 id="rag"><a class="markdownIt-Anchor" href="#rag"></a> <strong>▐  RAG</strong></h2><p>2020 年，Facebook（后更名为 Meta）在 “Retrieval-Augmented Generation for Knowledge-Intensive NLPTasks” 一文中首先提出了一种称为检索增强生成 (RAG) 的框架。该框架可以使模型访问超出其训练数据范围之外的信息，使得模型在每次生成时可以利用检索提供的外部更专业、更准确的知识，从而更好地回答用户问题。</p><p>在 RAG 系统中，模型可以通过浏览外部知识来回答用户的问题，而不是试图从参数记忆中找到问题相关的答案，就像在考试的时候是开卷考试还是闭卷考试一样。例如：我们可以分别询问 ChatGPT 和 Bing Chat 两个问题：“RAG 是什么？”“为什么大模型都是 Decoder（解码器）结构？” 因为 Bing Chat 可以结合互联网的搜索数据来生成答案，所以答案会更精准并且信息量更足。</p><h2 id="工作流程-2"><a class="markdownIt-Anchor" href="#工作流程-2"></a> <strong>▐  工作流程</strong></h2><p>RAG 的工作流程涉及 3 个主要阶段：数据准备、数据召回和答案生成。数据准备阶段包括识别数据源、从数据源提取数据、清洗数据并将其存储在数据库中。数据召回阶段包括根据用户输入的查询条件从数据库中检索相关数据。答案生成阶段则是利用检索到的数据和用户输入的查询条件生成输出结果。输出质量的高低取决于数据质量和检索策略。</p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="8a6aecba-a54c-4740-8f3e-42844074776c.png" alt=""></p><ul><li><h4 id="数据准备"><a class="markdownIt-Anchor" href="#数据准备"></a> 数据准备</h4></li></ul><p>根据 LLM 需要处理的任务类型，数据准备通常包括识别数据源、从数据源中提取数据、清洗数据并将其存储在数据库中等环节。用于存储数据的数据库类型和准备数据的步骤可能会因应用场景和检索方法的不同而有所变化。例如，如果使用像 Faiss 这样的向量存储库，需要为数据创建嵌入并将其存储在向量存储库中；如果使用像 Elasticsearch 这样的搜索引擎，需要将数据索引到搜索引擎中；如果使用像 Neo4j 这样的图数据库，需要为数据创建节点和边，并将它们存储到图数据库中。</p><ul><li><h4 id="数据召回"><a class="markdownIt-Anchor" href="#数据召回"></a> 数据召回</h4></li></ul><p>数据召回部分的主要任务是从大型文本数据库中检索与输入关的信息。为了尽可能保证正确答案被送入生成器部分，数据召回部分的召回率显得非常重要。一般来说，召回的数量越大，正确答案被召回的概率也就越高，但同时会面临大模型上下文长度限制的问题。</p><p>许多开源博客或框架在这部分的流程中都采用向量搜索出最相近的 k 个候选。例如，如果我们正在构建一个问答系统，并使用向量数据库存储相关数据块，可以为用户的问题生成向量，对向量数据库中的向量进行相似性搜索并检索最相似的数据块。除此之外，还可以根据用户问题，对同一数据库进行混合搜索或使用多个数据库进行搜索，并将结果组合起来作为生成器的上下文进行传递。</p><p>关于检索这部分，还有许多提高检索效果的技巧，这会引入更多的小模块，例如候选重排、大模型辅助召回等，这些都属于数据检索的范畴。</p><ul><li><h4 id="答案生成"><a class="markdownIt-Anchor" href="#答案生成"></a> 答案生成</h4></li></ul><p>一旦检索到用户问题相关的数据片段，RAG 系统就将其与用户的问题和相关数据一起传递给生成器 (LLM)。LLM 利用检索到的数据和用户的查询或任务生成输出。输出的质量取决于数据的质量和检索策略，同时生成输出的指令也会极大地影响输出的质量。</p><ul><li>RAG 的优缺点</li></ul><h4 id="rag-的优点"><a class="markdownIt-Anchor" href="#rag-的优点"></a> RAG 的优点</h4><p>前面介绍了 RAG 的基础内容，下面来具体梳理一下 RAG 的优点。</p><h4 id="高质量的答案生成降低答案生成的幻觉"><a class="markdownIt-Anchor" href="#高质量的答案生成降低答案生成的幻觉"></a> <strong>高质量的答案生成，降低答案生成的幻觉</strong></h4><p>RAG 的一个优点是它能够生成高质量的回答。因为在生成过程中，检索器可以从大量文档中检索问题相关的信息，然后基于这些信息生成回答。这使得整个系统能够充分利用现有知识生成更准确、更具深度的回答，也意味着模型出现幻觉答案的概率更小。</p><h4 id="可扩展性"><a class="markdownIt-Anchor" href="#可扩展性"></a> <strong>可扩展性</strong></h4><p>RAG 展示了出色的可扩展性，这意味着它能够轻松适应新数据和任务。利用 RAG 的检索 — 生成框架，只需更新检索部分的数据，模型便可适应新的知识领域。这使得 RAG 能够在面对新领域或不断变化的知识库时保持高度的适应性。</p><h4 id="模型可解释性"><a class="markdownIt-Anchor" href="#模型可解释性"></a> <strong>模型可解释性</strong></h4><p>RAG 具有一定程度的可解释性，这意味着我们可以理解模型是如何生成回答的。由于 RAG 的特性，我们可以很容易地追溯模型是从哪些文档中提取信息的。这使得我们可以评估模型的回答是否基于可靠的数据来源，从而提高模型的可信度。</p><h4 id="成本效益"><a class="markdownIt-Anchor" href="#成本效益"></a> <strong>成本效益</strong></h4><p>由于 RAG 的知识库能够与生成模型解耦，因此只要拥有一定的数据量，企业便可将 RAG 作为微调的替代方法，而微调可能需要大量资源。这种模式对中小企业非常友好。从另一个角度来看，由于企业的数据都是私有的，提供相关文档作为背景信息可以使生成结果更加准确、更具实用性，以满足企业的特定任务需求。</p><h4 id="rag-的缺点"><a class="markdownIt-Anchor" href="#rag-的缺点"></a> RAG 的缺点</h4><h4 id="依赖于检索模块"><a class="markdownIt-Anchor" href="#依赖于检索模块"></a> <strong>依赖于检索模块</strong></h4><p>RAG 系统给出的答案极其依赖于检索的质量。如果检索到的文档与问题无关或质量较低，生成的回答也可能质量较低。如果搜索的文档并未覆盖到问题的答案，那模型也基本无法回答用户提出的问题。因此，在实际应用中，我们会利用很多策略来提高文档片段的召回率。在很多场景中，文档片段的时效性也是要考虑的一部分，例如金融场景，用户咨询 10 月份的金股是什么，如果召回片段不包含 10 月份的券商金股研报，甚至召回很多旧的金股研报，那对最后的大模型生成会产生很大的干扰。还有很多其他的召回情况都会影响到模型的结果生成，因此想构建一个好的 RAG 系统，检索部分是极其重要的，需要花费大量的时间来打磨。</p><h4 id="依赖于现有的知识库"><a class="markdownIt-Anchor" href="#依赖于现有的知识库"></a> <strong>依赖于现有的知识库</strong></h4><p>RAG 依赖于一个现有的文档数据库进行检索。首先，如果没有一个大规模的知识库，就无法发挥 RAG 的优点。其次，如果知识库覆盖面不够，无法召回相应的知识块，那么模型因为需要遵循指令的约束而无法给出答案，这就会影响到整个系统的问题覆盖率。</p><h4 id="推理耗时"><a class="markdownIt-Anchor" href="#推理耗时"></a> <strong>推理耗时</strong></h4><p>由于 RAG 系统需要先检索文档，然后生成答案，相比于纯粹的大模型推理，整个系统的推理耗时会更长。在这种情况下，对于一些延时要求高的场景就无法满足需求。不过这个耗时问题属于大模型的通病，在使用网页端 ChatGPT 的时候，它以流式打字机的模式展示并按字来输出结果，所以用户可能不会感觉很慢，但如果统计从问题发送到答案完整生成这个过程，耗时还是非常长的。</p><h4 id="上下文窗口限制"><a class="markdownIt-Anchor" href="#上下文窗口限制"></a> <strong>上下文窗口限制</strong></h4><p>召回模块输出的文档片段数量需要考虑到生成模型能处理的最大长度，例如最早的 ChatGPT（GPT-3.5-turbo）的最大上下文长度是 4096 个 token。如果你的文档片段是 512 个 token 的长度，那实际上需要使用 8 个片段（512×8 = 4096），所以召回部分就需要考虑如何在这 8 个片段中把召回率做到最优。不过也有其他的折中方案，可以召回更多的文档片段。例如，可以采用对检索的文档片段进行压缩，借助大模型进行要点总结之类的策略。也可以对生成端的模型应用长度外推技巧，现有的长度外推策略已经比较成熟，有很多非常优秀的外推策略，可以让模型推理的长度远远超过训练阶段的长度。</p><h2 id="提示词工程"><a class="markdownIt-Anchor" href="#提示词工程"></a> <strong>▐  提示词工程</strong></h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="159e3dbf-b690-4f26-86ad-9e6ab562dd91.png" alt=""></p><p>提示词工程（Prompt Engineering）是一种在人工智能和自然语言处理领域中开发和设计提示词（Prompts）以引导大型语言模型（例如 GPT-3 等）产生特定输出的方法。通过精心构建和优化提示词，用户可以更有效地获得所需的答案、生成文本或执行其他自然语言处理任务。</p><p>提示词工程的关键在于找到合适的语言和结构来清晰地表达问题或任务，使得模型可以更准确地理解并给出相关的回应。这可能涉及反复试验、调整提示词的细节，以及利用对模型行为的理解来优化结果。接下来通过一些基础案例介绍如何优化 Prompt 使得大模型更好的回答我们的问题。</p><p>描述答案的标注</p><p>在与 LLM 交互时，最好在提示中清楚地描述所期望的答案标准。不要假设 LLM 具有与人类相似的理解能力，也不要期望它一定会以人类的方式进行回答。与 LLM 交互时使用 prompt 通常会显得有些 “啰嗦”，这是正常的。但要注意每一条关于答案标准的描述都应与所期望实现的目标密切相关，避免冗余信息，以降低 LLM 理解的难度。<br>比如我们可以在提问 “北京有哪些景点” 时增加 “请不要过多介绍景点” 来简化 ChatGPT 的输出结果。</p><p>设置兜底的回答方式</p><p>在某些情况下，向量化模型可能无法准确召回与用户问题相关的文本，甚至与用户问题几乎没有任何关联。如果让 LLM 根据这些召回的文本生成答案，可能会得到与问题无关或不符合事实的答案。因此，我们需要明确告知 LLM，如果上下文中没有与用户问题相关的答案，就不要强行生成答案了。这样能够避免产生不准确或不相关的回答。</p><p>输入中提供问答实例</p><p>有时候，我们很难通过语言准确地描述一项任务。如果任务描述不清楚，或者问题本身很复杂，会导致语言模型产生歧义，进而严重影响回答的效果。遇到这种情况，可以尝试在输入中增加一些问答示例，让语言模型自行领悟接下来应该做的任务。一般情况下，为语言模型提供问答示例是有益的。然而，需要注意的是，示例的数量、顺序以及与真实问题的相关性等因素都会影响语言模型的回答效果，这需要大量实践来确定。在提供示例时，尽量涵盖简单、困难、长尾等各种类型的示例。</p><p>标识出 prompt 中不同类型的内容</p><p>在撰写 prompt 时，最好能把任务描述、示例、引用文本等不同类型的内容用特殊符号隔开，避免 LLM 在内容理解上有歧义，同时也便于用户对 prompt 进行修改与维护。如果用户下达的指令和 prompt 其他内容是冲突的，使用该技巧就十分重要了。</p><p><strong>设定输出格式</strong></p><p>ChatGPT 等模型都是经过对话数据微调的。在需要准确输出内容点的场景中，有时可能会出现输出无用信息或过于口语化的情况，这不利于进一步提取输出中所需的内容。一种有效的解决方法是让 LLM 以 json 格式输出内容，如果效果不佳，也可以尝试在提示中增加输出 json 的示例。有时候 LLM 输出的 json 格式可能不够标准（例如，字典的键值没有引号或冒号采用中文格式），不能直接使用 Python 的 json 包进行处理，则可以借助正则表达式进行处理。如果不熟悉正则表达式，可直接向 ChatGPT 询问。</p><p>指定大模型的身份</p><p>在 prompt 中，告诉 LLM 所扮演的身份是什么，这可以帮助 LLM 确定接下来输出的内容和说话风格。</p><p>使用思维链</p><p>对 于 LLM 来说，当它进行推理相关的任务时，要求它输出推理过程同样可以减少错误的发生。以计算一个简单的数学题为例，直接输出的答案可能是错误的。只有要求它逐步给出每一步的计算过程，才能得到正确的答案。LLM 不仅在训练时可以通过增加计算量（训练更多数据）来提高效果，而且在推理过程中也可以做到这一点（输出更多内容）。</p><p><strong>▐  模型微调</strong></p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="b870789c-e605-4ac5-9313-8ec49e92eb77.jpg" alt=""></p><p>目前通用的大语言模型，比如 ChatGPT，Gemini，通义千问等事实上都属于预训练 (Pre-trained) 模型，预训练是指通过互联网上已知的海量的语料对原始大模型进行训练，训练后的大模型具有了通用领域知识的解答和推理能力。而对于拥有特定领域知识的企业来说，想要落地大模型往往需要大模型能够理解企业的领域知识且私有数据不能被泄露。所以在企业落地的实际应用中需要对大模型进行微调。</p><p>基于已有开源大模型进行微调训练，如果采用预训练的方式对模型的所有参数都进行训练微调，由于现有的开源模型参数量都十分巨大，如最新千问 72B 模型 (qwen/Qwen1.5-72B-Chat) 有 720 亿参数，对所有的参数都进行处理，那 GPU 资源成本会非常高，可能高达数百万每年，为了解决这个问题，社区提出了大模型微调的概念：PEFT (Parameter-Efficient Fine-Tuning)，即对开源预训练模型的所有参数中的一小部分参数进行训练微调，最后输出的结果和全参数微调训练的效果接近。</p><p>PEFT 的基本思想是保持大部分参数不变，通过微调一小部分参数，达到具有竞争力甚至是领先的性能。由于需要更新的参数量小，其所需的数据和算力资源变小，使得微调更加有效率。<br>下面介绍最常见的高效微调方法 LoRA 以及他的一些变体：</p><p>LoRA（Low-Rank Adaptation）</p><p>LoRA 即 LLMs 的低秩适应，是参数高效微调最常用的方法。LoRA 的本质就是用更少的训练参数来近似 LLM 全参数微调所得的增量参数，从而达到使用更少显存占用的高效微调。<br>简单来说，LoRA 在模型参数矩阵的（例如 m*n 维）旁边新增一支旁路，由两个低秩矩阵（m*r, r*n 维，r 远小于 m,n）相乘，前向过程中，同时经过原始矩阵和旁路（LoRA 部分），分别得到各自的输出再相加起来，训练时冻结原始参数，只训练 LoRA，由于 LoRA 部分是两个低秩矩阵，参数量远远小于原始矩阵，因此可以显著减少训练代价。</p><p><strong>QLoRA（Quantized LoRA）</strong></p><p>QLoRA 是模型量化和 LoRA 的结合。除了增加了 LoRA 旁路，QLoRA 在加载时将大模型量化成 4bit 或者 8bit，但在计算时，又将该部分的参数反量化成 16bit 进行计算，其优化了非使用状态模型参数的存储，与 LoRA 相比进一步降低训练时显存消耗。</p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="602afcb6-4ffc-4c0d-b6b5-1de5d31f30a2.png" alt=""></p><p><strong>大模型框架</strong></p><p>大型语言模型 (LLM) 如 GPT 系列模型引领了人工智能领域的一场技术革命。开发者们都在利用这些 LLM 进行各种尝试，虽然已经产生了许多有趣的应用，但是单独使用这些 LLM 往往难以构建功能强大的实用应用。</p><p>也因此，大模型应用框架可以说是百花齐放，本文挑选了热度和讨论度较高的一些开源应用框架进行介绍。</p><h2 id="langchain"><a class="markdownIt-Anchor" href="#langchain"></a> <strong>▐  LangChain</strong></h2><p>LangChain 是一个开源的应用开发框架，目前支持 Python 和 TypeScript 两种编程语言。它赋予 LLM 两大核心能力：数据感知，将语言模型与其他数据源相连接；代理能力，允许语言模型与其环境互动。结合上述两种能力，LangChain 可以轻松管理与语言模型的交互，将多个组件链接在一起，并集成额外的资源。</p><p>通过 LangChain 可以实现围绕 LLM 核心快速构建 AI 应用，比如提示词工程、检索增强生成（RAG）、会话式 Agent 构建等。</p><p>LangChain 提供了一系列的工具帮助我们更好的使用大语言模型 (LLM)。主要有 6 种不同类型的工具：</p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="56b8f589-12f6-4fec-b352-84be7f94f2d6.png" alt=""></p><ol><li><p>模型（models） : LangChain 支持的模型类型包括 LLM、TextEmbedding，模型集成如 GPT-4、Llama 等。</p></li><li><p>提示（prompts） : 包括 prompt 模板管理、优化。</p></li><li><p>链（chains） : 链不仅仅是单个 LLM 调用，还包括一系列包含 LLM、工具、业务 api 等的调用。LangChain 提供了标准的链接，以及一些常见的链实现，可快速实现应用程序的端到端的链调用。</p></li><li><p>索引（indexes） : 索引是连接知识与 LLM 的关键，可构建属于企业、场景、用户个人的专属知识库。主要包括文档加载、分割、向量化、检索，链中使用索引的常见方式是 “检索”，如 RAG 一般都会经过 “检索” 步骤，以获取与用户问题最接近的可靠知识信息。</p></li><li><p>代理（agents） : 也可以称之为智能体，简而言之是某个人或场景的代理人，相关的任务都可以通过 Agent 完成。LangChain 提供了一个标准的代理接口，模拟人的通用思维链路，从作出决策到执行行动、观察结果，再重复，直至任务完成。开发者为 agent 提供一套工具，agent 根据任务描述自行决策和编排使用何种工具完成目标任务。</p></li><li><p>记忆（memory） : LLM、chain、agent 默认均是无状态的，即 LLM 会独立的处理每次输入。而记忆是人类智能的必要能力之一，不论长短期记忆，对后续的交互都非常重要。在 LangChain 中，记忆是在链 / 代理调用之间保持状态的概念，其提供了标准的记忆接口及部分实现。</p></li></ol></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://www.formeasy.cc">formeasy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://www.formeasy.cc/2025/04/05/LLM/%E6%8A%80%E6%9C%AF%E4%BA%BA%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%88%9D%E5%AD%A6%E6%8C%87%E5%8D%97/">http://www.formeasy.cc/2025/04/05/LLM/技术人的大模型应用初学指南/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://www.formeasy.cc" target="_blank">易锦风的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/2025/04/05/LLM/%E6%8A%80%E6%9C%AF%E4%BA%BA%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E5%88%9D%E5%AD%A6%E6%8C%87%E5%8D%97/e74ce26e-86d9-4380-881b-c7d75ed44e12.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/04/08/Python/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83venv%E7%9B%B4%E6%8E%A5%E5%A4%8D%E5%88%B6%E8%BF%81%E7%A7%BB%E7%9A%84%E6%96%B9%E6%B3%95/" title="python虚拟环境venv直接复制迁移的方法"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/04/08/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83venv%E7%9B%B4%E6%8E%A5%E5%A4%8D%E5%88%B6%E8%BF%81%E7%A7%BB%E7%9A%84%E6%96%B9%E6%B3%95/2561d6a86a203d8b4bd82a7ab8261fb8.jpeg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">python虚拟环境venv直接复制迁移的方法</div></div><div class="info-2"><div class="info-item-1">python虚拟环境venv迁移布署有很多的方法，大家自行搜索。这里只介绍一种：直接复制的方法 1、将整个文件夹复制到新的电脑 2、修改pyvenv.cfg文件内的home为你新电脑python的安装路径。 3、 如果你使用vscode，还需要修改vscode的配置文件launch.json，这样就可以在新的电脑上调试了。 4、修改程序目录下Scripts\activate文件（可以用记事本打开） VIRTUAL_ENV=&quot;E:\your_folder&quot;改为你新电脑的位置 5、修改程序目录下Scripts\activate.bat文件（可以用记事本打开） set VIRTUAL_ENV=E:\mySourse\anzhi4改为你新电脑的位置 6、运行程序目录下Scripts\activate.bat文件，激活虚拟环境。</div></div></div></a><a class="pagination-related" href="/2025/04/05/Other/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E7%9A%8413%E6%9D%A1%E6%B3%95%E5%88%99/" title="软件工程的 13 条法则"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/04/05/Other/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B%E7%9A%8413%E6%9D%A1%E6%B3%95%E5%88%99/up-b6e9c01e7725c6b53b7800033fffdaf1f3f.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">软件工程的 13 条法则</div></div><div class="info-2"><div class="info-item-1">1、帕金森定律：工作会膨胀以填满可用的时间。 2、霍夫斯塔特定律：事情总是比你预期的要长，即使你已经考虑了霍夫斯塔特定律。 3、布鲁克斯定律：向一个已经延期的软件项目增加人力只会让它更加延期。 4、康威定律（及逆康威定律）：组织做的设计往往是其内部沟通结构的复制品。 5、坎宁安定律：在互联网上获得正确答案的最佳方式不是提问，而是发布一个错误答案。 6、斯特金定律：90% 的东西都是垃圾。 7、扎温斯基定律：每个程序都试图扩展，直到能够读取邮件。那些无法如此扩展的程序会被能够做到的程序所取代。 8、海勒姆定律：当 API 的用户数量足够多时，你在合约中承诺什么并不重要：系统的所有可观察行为都会被某些人所依赖。 9、普赖斯定律：在任何群体中，50%...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/08/25/LLM/AI%E6%8E%A2%E7%B4%A2%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E7%9A%84%E5%88%86%E7%B1%BB%EF%BC%9AChat%E3%80%81Code%E3%80%81Embedding%E5%92%8CRerank/" title="AI探索大模型权重的分类：Chat、Code、Embedding和Rerank"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/08/25/LLM/AI%E6%8E%A2%E7%B4%A2%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E7%9A%84%E5%88%86%E7%B1%BB%EF%BC%9AChat%E3%80%81Code%E3%80%81Embedding%E5%92%8CRerank/ca10ab6a4e4404fde0c4560c150f4d26.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-08-25</div><div class="info-item-2">AI探索大模型权重的分类：Chat、Code、Embedding和Rerank</div></div><div class="info-2"><div class="info-item-1">在机器学习和自然语言处理领域，大模型（如GPT-3、BERT等）已经成为了强大且广泛应用的工具。大模型的权重通常可以根据其应用场景分为不同的类别，如Chat、Code、Embedding和Rerank。了解这些分类及其差异对于我们在实际应用中选择合适的模型至关重要。本文将详细讲解这四种权重分类，并说明它们的差异。 1. Chat（对话） Chat模型专注于对话生成和自然语言理解。这些模型经过专门训练，能够理解并生成连贯、自然的对话。Chat模型通常用于客服机器人、虚拟助手等场景。 特点： 自然语言生成：能够生成流畅且有意义的对话。 上下文理解：能够记住对话的上下文并进行相关的回答。 人性化交互：与用户进行类人互动，提供友好的用户体验。 应用场景： 在线客服 智能助手（如Siri、Alexa） 社交媒体聊天机器人 2....</div></div></div></a><a class="pagination-related" href="/2025/03/21/LLM/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%88%86%E7%B1%BB%E6%A6%82%E5%86%B5_%E5%90%91%E9%87%8F%E5%BA%93%E7%B1%BB%E5%9E%8B/" title="向量数据库的分类概况_向量库类型"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/03/21/LLM/%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%88%86%E7%B1%BB%E6%A6%82%E5%86%B5_%E5%90%91%E9%87%8F%E5%BA%93%E7%B1%BB%E5%9E%8B/163d5fc517dc58b53456f98deafed0db.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-21</div><div class="info-item-2">向量数据库的分类概况_向量库类型</div></div><div class="info-2"><div class="info-item-1">向量数据库的分类概况 保存和检索矢量数据的五种方法： 像 Pinecone 这样的纯矢量数据库 全文搜索数据库，例如 ElasticSearch 矢量库，如 Faiss、Annoy 和 Hnswlib 支持矢量的NoSQL 数据库，例如 MongoDB、Cosmos DB 和 Cassandra 支持矢量的SQL 数据库，例如 SingleStoreDB 或 PostgreSQL 1.纯矢量数据库 纯向量数据库专门用于存储和检索向量。示例包括 Chroma、LanceDB、Marqo、Milvus/Zilliz、Pinecone、Qdrant、Vald、Vespa、Weaviate 等。 在纯矢量数据库中，数据是根据对象或数据点的矢量表示来组织和索引的。这些向量可以是各种类型数据的数值表示，包括图像、文本文档、音频文件或任何其他形式的结构化或非结构化数据。 纯载体数据库的优点 利用索引技术进行高效的相似性搜索 大型数据集和高查询工作负载的可扩展性 支持高维数据 支持基于 HTTP 和 JSON 的...</div></div></div></a><a class="pagination-related" href="/2025/07/29/LLM/%E7%90%86%E8%A7%A3%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%EF%BC%88Fine-tuning%EF%BC%89%20%E5%92%8C%20%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F%EF%BC%88Distillation%EF%BC%89/" title="理解模型微调（Fine-tuning） 和 模型蒸馏（Distillation）"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-29</div><div class="info-item-2">理解模型微调（Fine-tuning） 和 模型蒸馏（Distillation）</div></div><div class="info-2"><div class="info-item-1">大模型蒸馏与大模型微调是当前人工智能领域中两种重要的技术手段，它们在模型优化、性能提升和资源利用方面各有特点。以下将从定义、技术原理、应用场景及优缺点等方面对这两种技术进行深入对比。 一、定义与基本概念 大模型蒸馏（Knowledge Distillation） 蒸馏是一种将大型复杂模型（教师模型）的知识迁移到小型模型（学生模型）的技术。通过训练学生模型模仿教师模型的行为，实现模型压缩和性能保留的目标。蒸馏过程通常包括两个阶段：预训练阶段（教师模型训练）和知识传递阶段（学生模型训练）。 大模型微调（Fine-tuning） 微调是指在预训练的大模型基础上，通过少量标注数据的再训练，使模型适应特定任务的需求。微调可以分为全量微调和参数高效微调（如PEFT）。全量微调适用于需要高精度输出的任务，而参数高效微调则通过优化超参数和调整策略，减少计算资源消耗。 ...</div></div></div></a><a class="pagination-related" href="/2025/05/28/LLM/%E9%9D%A0%E8%BF%99%E4%B9%9D%E6%AC%BE%E5%9B%BD%E4%BA%A7AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%B0%B1%E5%AE%9E%E7%8E%B0%E4%BA%866%E4%BA%BF%E4%BA%BA%E7%9A%84AI%E6%A2%A6/" title="靠这九款国产AI大模型，就实现了6亿人的AI梦"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/05/28/LLM/%E9%9D%A0%E8%BF%99%E4%B9%9D%E6%AC%BE%E5%9B%BD%E4%BA%A7AI%E5%A4%A7%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%B0%B1%E5%AE%9E%E7%8E%B0%E4%BA%866%E4%BA%BF%E4%BA%BA%E7%9A%84AI%E6%A2%A6/14b4cc0530003f26b009ab8bb22bd279.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-28</div><div class="info-item-2">靠这九款国产AI大模型，就实现了6亿人的AI梦</div></div><div class="info-2"><div class="info-item-1">2024年 10 月 13 日，工业和信息化部总工程师，赵立国发表讲话：“我国人工智能核心产业的规模在不断提升，企业数量超过了 4500 家，完成注册并提供服务的生成式人工智能服务大模型数量已经超过200个，注册用户数超过了 6 亿。” 这些大模型在各个领域中发挥着重要作用，推动了技术创新和产业变革。 可以看出 2024 年，中国的人工智能技术发展速度真可谓是日新月异，国产AI大模型如雨后春笋般涌现，从 1 月份的 80 多个到 10 月份已经突破超过 200 个，不到十个月时间就激增了 100 多个大模型。 这些大模型中主要以通用大模型 Kimi、智谱清言、通义千问、文心一言、豆包、天工AI、讯飞星火、秘塔和腾讯元宝这九大模型格外引人注目。 在科技大国走向科技强国的号召下，AI 大模型的重要性尤其凸显，这九款国产大模型发展相当出色，它们铸就了中国人我们中国人的 AI 梦。 下面，让我们一起探索这些大模型的特点和面临的挑战，普及和了解它们在AI领域的地位和潜力。 ...</div></div></div></a></div></div><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/butterfly-icon.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">formeasy</div><div class="author-info-description">专注互联网和软件技术</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">213</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">38</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/formeasy"><i class="fab fa-github"></i><span>关注我</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/formeasy" target="_blank" title="Github"><i class="fab fa-github" style="color:#24292e"></i></a><a class="social-icon" href="mailto:formeasy@163.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color:#4a7dbe"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">用学习，面对遭遇的变化；用斗志，面对每天的挫折；用坚持，面对失去的动力!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#excerpt"><span class="toc-number">1.</span> <span class="toc-text">Excerpt</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">2.</span> <span class="toc-text">▐  向量数据库</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%AD%98%E5%82%A8%E5%92%8C%E7%B4%A2%E5%BC%95%E4%BC%98%E5%8C%96"><span class="toc-number">2.0.1.</span> <span class="toc-text">1. 存储和索引优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%8F%AC%E5%9B%9E%E7%8E%87%E4%BC%98%E5%8C%96"><span class="toc-number">2.0.2.</span> <span class="toc-text">2. 召回率优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%92%8C%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD"><span class="toc-number">2.0.3.</span> <span class="toc-text">3. 系统架构和基础设施</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E4%B8%93%E7%94%A8%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F"><span class="toc-number">2.0.4.</span> <span class="toc-text">4. 专用硬件加速</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E6%8C%81%E7%BB%AD%E4%BC%98%E5%8C%96%E5%92%8C%E6%9B%B4%E6%96%B0%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.0.5.</span> <span class="toc-text">5. 持续优化和更新模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-%E5%85%88%E8%BF%9B%E7%9A%84%E5%B5%8C%E5%85%A5%E6%8A%80%E6%9C%AF"><span class="toc-number">2.0.6.</span> <span class="toc-text">6. 先进的嵌入技术</span></a></li></ol></li></ol><li class="toc-item toc-level-2"><a class="toc-link" href="#multi-agent"><span class="toc-number">3.</span> <span class="toc-text">▐  Multi-Agent</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#conversable-agents"><span class="toc-number">3.0.1.</span> <span class="toc-text">Conversable Agents</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#conversation-programming"><span class="toc-number">3.0.2.</span> <span class="toc-text">Conversation Programming</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#metagpt-%E6%A1%86%E6%9E%B6"><span class="toc-number">3.0.3.</span> <span class="toc-text">MetaGPT 框架</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-number">3.0.4.</span> <span class="toc-text">工作流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%BE%E8%AE%A1%E5%B1%82%E6%AC%A1"><span class="toc-number">3.0.5.</span> <span class="toc-text">设计层次</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#rag"><span class="toc-number">4.</span> <span class="toc-text">▐  RAG</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B-2"><span class="toc-number">5.</span> <span class="toc-text">▐  工作流程</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="toc-number">5.0.1.</span> <span class="toc-text">数据准备</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8F%AC%E5%9B%9E"><span class="toc-number">5.0.2.</span> <span class="toc-text">数据召回</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AD%94%E6%A1%88%E7%94%9F%E6%88%90"><span class="toc-number">5.0.3.</span> <span class="toc-text">答案生成</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#rag-%E7%9A%84%E4%BC%98%E7%82%B9"><span class="toc-number">5.0.4.</span> <span class="toc-text">RAG 的优点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%AB%98%E8%B4%A8%E9%87%8F%E7%9A%84%E7%AD%94%E6%A1%88%E7%94%9F%E6%88%90%E9%99%8D%E4%BD%8E%E7%AD%94%E6%A1%88%E7%94%9F%E6%88%90%E7%9A%84%E5%B9%BB%E8%A7%89"><span class="toc-number">5.0.5.</span> <span class="toc-text">高质量的答案生成，降低答案生成的幻觉</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%AF%E6%89%A9%E5%B1%95%E6%80%A7"><span class="toc-number">5.0.6.</span> <span class="toc-text">可扩展性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%80%A7"><span class="toc-number">5.0.7.</span> <span class="toc-text">模型可解释性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%88%90%E6%9C%AC%E6%95%88%E7%9B%8A"><span class="toc-number">5.0.8.</span> <span class="toc-text">成本效益</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#rag-%E7%9A%84%E7%BC%BA%E7%82%B9"><span class="toc-number">5.0.9.</span> <span class="toc-text">RAG 的缺点</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BE%9D%E8%B5%96%E4%BA%8E%E6%A3%80%E7%B4%A2%E6%A8%A1%E5%9D%97"><span class="toc-number">5.0.10.</span> <span class="toc-text">依赖于检索模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BE%9D%E8%B5%96%E4%BA%8E%E7%8E%B0%E6%9C%89%E7%9A%84%E7%9F%A5%E8%AF%86%E5%BA%93"><span class="toc-number">5.0.11.</span> <span class="toc-text">依赖于现有的知识库</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8E%A8%E7%90%86%E8%80%97%E6%97%B6"><span class="toc-number">5.0.12.</span> <span class="toc-text">推理耗时</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AA%97%E5%8F%A3%E9%99%90%E5%88%B6"><span class="toc-number">5.0.13.</span> <span class="toc-text">上下文窗口限制</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B"><span class="toc-number">6.</span> <span class="toc-text">▐  提示词工程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#langchain"><span class="toc-number">7.</span> <span class="toc-text">▐  LangChain</span></a></li></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/09/17/Python/%E4%BD%BF%E7%94%A8WinSW%E6%8A%8Anginx%E5%81%9A%E6%88%90windows%E6%9C%8D%E5%8A%A1/" title="使用WinSW把nginx做成windows服务"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="使用WinSW把nginx做成windows服务"></a><div class="content"><a class="title" href="/2025/09/17/Python/%E4%BD%BF%E7%94%A8WinSW%E6%8A%8Anginx%E5%81%9A%E6%88%90windows%E6%9C%8D%E5%8A%A1/" title="使用WinSW把nginx做成windows服务">使用WinSW把nginx做成windows服务</a><time datetime="2025-09-17T05:58:13.000Z" title="发表于 2025-09-17 13:58:13">2025-09-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/09/17/Python/%E4%BD%BF%E7%94%A8NSSM%E5%B0%86.exe%E7%A8%8B%E5%BA%8F%E5%AE%89%E8%A3%85%E6%88%90windows%E6%9C%8D%E5%8A%A1/" title="使用NSSM将.exe程序安装成windows服务"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/09/17/Python/%E4%BD%BF%E7%94%A8NSSM%E5%B0%86.exe%E7%A8%8B%E5%BA%8F%E5%AE%89%E8%A3%85%E6%88%90windows%E6%9C%8D%E5%8A%A1/2447777-20240506104400143-1336746940.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="使用NSSM将.exe程序安装成windows服务"></a><div class="content"><a class="title" href="/2025/09/17/Python/%E4%BD%BF%E7%94%A8NSSM%E5%B0%86.exe%E7%A8%8B%E5%BA%8F%E5%AE%89%E8%A3%85%E6%88%90windows%E6%9C%8D%E5%8A%A1/" title="使用NSSM将.exe程序安装成windows服务">使用NSSM将.exe程序安装成windows服务</a><time datetime="2025-09-17T05:52:51.000Z" title="发表于 2025-09-17 13:52:51">2025-09-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/09/17/Python/python%E6%89%93%E5%8C%85flask%E6%9C%8D%E5%8A%A1%E6%88%90exe%E6%96%87%E4%BB%B6%E5%B9%B6%E5%BF%AB%E9%80%9F%E6%B3%A8%E5%86%8C%E6%88%90windows%E6%9C%8D%E5%8A%A1/" title="python打包flask服务成exe文件并快速注册成windows服务"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/09/17/Python/python%E6%89%93%E5%8C%85flask%E6%9C%8D%E5%8A%A1%E6%88%90exe%E6%96%87%E4%BB%B6%E5%B9%B6%E5%BF%AB%E9%80%9F%E6%B3%A8%E5%86%8C%E6%88%90windows%E6%9C%8D%E5%8A%A1/2b6f6336f903a8e1caad04fd7ead53a0.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="python打包flask服务成exe文件并快速注册成windows服务"></a><div class="content"><a class="title" href="/2025/09/17/Python/python%E6%89%93%E5%8C%85flask%E6%9C%8D%E5%8A%A1%E6%88%90exe%E6%96%87%E4%BB%B6%E5%B9%B6%E5%BF%AB%E9%80%9F%E6%B3%A8%E5%86%8C%E6%88%90windows%E6%9C%8D%E5%8A%A1/" title="python打包flask服务成exe文件并快速注册成windows服务">python打包flask服务成exe文件并快速注册成windows服务</a><time datetime="2025-09-17T05:46:01.000Z" title="发表于 2025-09-17 13:46:01">2025-09-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/09/14/Python/Windows%E4%B8%8A%E5%B0%86Eex%E9%83%A8%E7%BD%B2%E6%88%90%E4%B8%BA%E6%9C%8D%E5%8A%A1WinSW%E5%92%8CNSSM/" title="Windows上将Eex部署成为服务WinSW和NSSM"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/09/14/Python/Windows%E4%B8%8A%E5%B0%86Eex%E9%83%A8%E7%BD%B2%E6%88%90%E4%B8%BA%E6%9C%8D%E5%8A%A1WinSW%E5%92%8CNSSM/624386-20250122192200962-1765742953.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="Windows上将Eex部署成为服务WinSW和NSSM"></a><div class="content"><a class="title" href="/2025/09/14/Python/Windows%E4%B8%8A%E5%B0%86Eex%E9%83%A8%E7%BD%B2%E6%88%90%E4%B8%BA%E6%9C%8D%E5%8A%A1WinSW%E5%92%8CNSSM/" title="Windows上将Eex部署成为服务WinSW和NSSM">Windows上将Eex部署成为服务WinSW和NSSM</a><time datetime="2025-09-14T14:14:35.000Z" title="发表于 2025-09-14 22:14:35">2025-09-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/09/13/VUE/%E7%94%A8electron%E5%B0%86vue%E9%A1%B9%E7%9B%AE%E6%89%93%E5%8C%85%E6%88%90.exe%E6%96%87%E4%BB%B6%E3%80%90%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%95%99%E7%A8%8B%E3%80%91/" title="用electron将vue项目打包成.exe文件【保姆级教程】"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/09/13/VUE/%E7%94%A8electron%E5%B0%86vue%E9%A1%B9%E7%9B%AE%E6%89%93%E5%8C%85%E6%88%90.exe%E6%96%87%E4%BB%B6%E3%80%90%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%95%99%E7%A8%8B%E3%80%91/bbf8565071e2cf68eaa25eed66323d9e.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="用electron将vue项目打包成.exe文件【保姆级教程】"></a><div class="content"><a class="title" href="/2025/09/13/VUE/%E7%94%A8electron%E5%B0%86vue%E9%A1%B9%E7%9B%AE%E6%89%93%E5%8C%85%E6%88%90.exe%E6%96%87%E4%BB%B6%E3%80%90%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%95%99%E7%A8%8B%E3%80%91/" title="用electron将vue项目打包成.exe文件【保姆级教程】">用electron将vue项目打包成.exe文件【保姆级教程】</a><time datetime="2025-09-13T13:45:49.000Z" title="发表于 2025-09-13 21:45:49">2025-09-13</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By formeasy</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(async()=>{window.katex_js_css||(window.katex_js_css=!0,await btf.getCSS("https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"),await btf.getScript("https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js")),document.querySelectorAll("#article-container .katex").forEach(t=>t.classList.add("katex-show"))})()</script><script>(()=>{var e=()=>{var e;0!==(e=document.querySelectorAll("pre > code.mermaid")).length&&e.forEach(e=>{var t=document.createElement("pre"),n=(t.className="mermaid-src",t.hidden=!0,t.textContent=e.textContent,document.createElement("div"));n.className="mermaid-wrap",n.appendChild(t),e.parentNode.replaceWith(n)});const t=document.querySelectorAll("#article-container .mermaid-wrap");0!==t.length&&(e=()=>{{var e=t;window.loadMermaid=!0;const a="dark"===document.documentElement.getAttribute("data-theme")?"dark":"default";e.forEach((e,t)=>{const n=e.firstElementChild;e=`%%{init:{ 'theme':'${a}'}}%%
`+n.textContent,t=mermaid.render("mermaid-"+t,e);const d=e=>{n.insertAdjacentHTML("afterend",e)};"string"==typeof t?d(t):t.then(({svg:e})=>d(e))})}},btf.addGlobalFn("themeChange",e,"mermaid"),window.loadMermaid?e():btf.getScript("https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js").then(e))};btf.addGlobalFn("encrypt",e,"mermaid"),window.pjax?e():document.addEventListener("DOMContentLoaded",e)})()</script><script>(()=>{const a=GLOBAL_CONFIG_SITE.isShuoshuo,s=null,n=t=>"dark"===t?"dark":"light";var t=(t=document,e)=>{e=a?{"data-mapping":"specific","data-term":e}:{"data-mapping":(s,"pathname")},e=(t=>{const a=document.createElement("script");return Object.entries(t).forEach(([t,e])=>{a.setAttribute(t,e)}),a})({src:"https://giscus.app/client.js","data-repo":"formeasy/blog","data-repo-id":"R_kgDOJ2sA5A","data-category-id":"DIC_kwDOJ2sA5M4CktKh","data-theme":n(document.documentElement.getAttribute("data-theme")),"data-reactions-enabled":"1",crossorigin:"anonymous",async:!0,...s,...e});t.querySelector("#giscus-wrap").appendChild(e),a&&(window.shuoshuoComment.destroyGiscus=()=>{t.children.length&&(t.innerHTML="",t.classList.add("no-comment"))})};btf.addGlobalFn("themeChange",t=>{var e=document.querySelector("#giscus-wrap iframe");e&&(t={giscus:{setConfig:{theme:n(t)}}},e.contentWindow.postMessage(t,"https://giscus.app"))},"giscus"),a?window.shuoshuoComment={loadComment:t}:t()})()</script></div><script defer id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zindex="-1" mobile="false" data-click="true"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span> 数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="请输入搜索关键字" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>