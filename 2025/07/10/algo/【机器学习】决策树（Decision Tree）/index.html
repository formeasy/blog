<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>【机器学习】决策树（Decision Tree） | 易锦风的博客</title><meta name="author" content="formeasy"><meta name="copyright" content="formeasy"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="【机器学习】决策树（Decision Tree）"><meta property="og:type" content="article"><meta property="og:title" content="【机器学习】决策树（Decision Tree）"><meta property="og:url" content="http://www.formeasy.cc/2025/07/10/algo/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%91%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%88Decision%20Tree%EF%BC%89/index.html"><meta property="og:site_name" content="易锦风的博客"><meta property="og:description" content="【机器学习】决策树（Decision Tree）"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://www.formeasy.cc/2025/07/10/algo/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%91%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%88Decision%20Tree%EF%BC%89/797984871ce756bea69a8e3cbcde1c5d.png"><meta property="article:published_time" content="2025-07-10T01:33:12.000Z"><meta property="article:modified_time" content="2025-07-10T01:38:24.844Z"><meta property="article:author" content="formeasy"><meta property="article:tag" content="algo"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://www.formeasy.cc/2025/07/10/algo/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%91%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%88Decision%20Tree%EF%BC%89/797984871ce756bea69a8e3cbcde1c5d.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://www.formeasy.cc/2025/07/10/algo/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%91%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%88Decision%20Tree%EF%BC%89/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>(()=>{var e={set:(e,t,a)=>{a&&(a=Date.now()+864e5*a,localStorage.setItem(e,JSON.stringify({value:t,expiry:a})))},get:e=>{var t=localStorage.getItem(e);if(t){var{value:t,expiry:a}=JSON.parse(t);if(!(Date.now()>a))return t;localStorage.removeItem(e)}}},t=(window.btf={saveToLocal:e,getScript:(o,n={})=>new Promise((e,t)=>{const a=document.createElement("script");a.src=o,a.async=!0,Object.entries(n).forEach(([e,t])=>a.setAttribute(e,t)),a.onload=a.onreadystatechange=()=>{a.readyState&&!/loaded|complete/.test(a.readyState)||e()},a.onerror=t,document.head.appendChild(a)}),getCSS:(o,n)=>new Promise((e,t)=>{const a=document.createElement("link");a.rel="stylesheet",a.href=o,n&&(a.id=n),a.onload=a.onreadystatechange=()=>{a.readyState&&!/loaded|complete/.test(a.readyState)||e()},a.onerror=t,document.head.appendChild(a)}),addGlobalFn:(e,t,a=!1,o=window)=>{var n;e.startsWith("pjax")||((n=o.globalFn||{})[e]=n[e]||{},n[e][a||Object.keys(n[e]).length]=t,o.globalFn=n)}},()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")}),a=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")},o=(btf.activateDarkMode=t,btf.activateLightMode=a,e.get("theme")),t=("dark"===o?t():"light"===o&&a(),e.get("aside-status"));void 0!==t&&document.documentElement.classList.toggle("hide-aside","hide"===t);/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"/search.xml",preload:!1,top_n_per_article:1,unescape:!1,languages:{hits_empty:"未找到符合您查询的内容：${query}",hits_stats:"共找到 ${hits} 篇文章"}},translate:void 0,highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1,highlightFullpage:!1,highlightMacStyle:!0},copy:{success:"复制成功",error:"复制失败",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"【机器学习】决策树（Decision Tree）",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,isShuoshuo:!1}</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="易锦风的博客" type="application/atom+xml"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/butterfly-icon.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">242</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">39</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我们</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(/2025/07/10/algo/【机器学习】决策树（Decision%20Tree）/797984871ce756bea69a8e3cbcde1c5d.png)"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><img class="site-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/icon.png" alt="Logo"><span class="site-name">易锦风的博客</span></a><a class="nav-page-title" href="/"><span class="site-name">【机器学习】决策树（Decision Tree）</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我们</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">【机器学习】决策树（Decision Tree）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-07-10T01:33:12.000Z" title="发表于 2025-07-10 09:33:12">2025-07-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-10T01:38:24.844Z" title="更新于 2025-07-10 09:38:24">2025-07-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%99%BA%E8%83%BD/">智能</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">3.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>9分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h3 id="引入"><a class="markdownIt-Anchor" href="#引入"></a> 引入</h3><p>决策树是一类预测模型，它代表的是对象属性与对象值之间的一种映射关系。树中每个节点表示某个对象，而每个分叉路径则代表某个可能的属性值，而每个叶节点则对应从根节点到该叶节点所经历的路径所表示的对象的值。</p><h3 id="关于分类问题"><a class="markdownIt-Anchor" href="#关于分类问题"></a> 关于分类问题</h3><p>这里主要考虑决策树基于分类问题的处理算法，分类问题和回归问题有个简单的判别方式：分类的目标属性是离散的，而回归的目标属性是连续的。</p><h3 id="分类问题的步骤"><a class="markdownIt-Anchor" href="#分类问题的步骤"></a> 分类问题的步骤</h3><p>1、模型构建：通过对训练集合的归纳，利用归纳算法生成可读的规则，建立分类模型。</p><p>2、预测推论：根据规则和建立的分类模型，对测试集合进行测试，并处理新的数据。</p><h3><a class="markdownIt-Anchor" href="#"></a></h3><h3 id="关于归纳算法"><a class="markdownIt-Anchor" href="#关于归纳算法"></a> 关于归纳算法</h3><ul><li>归纳是从特殊到一般的过程，归纳过程就是在描述空间中进行搜索的过程。</li><li>归纳可分为自顶向下，自底向上和双向搜索三种方式。<ul><li>自底向上法：一次处理一个输入对象，将描述逐步一般化， 直到最终的一般化描述。</li><li>自顶向下法：对一般性描述集进行搜索，寻找满足一定要求的最优的描述。</li></ul></li><li>归纳算法是决策树技术发现数据模式和规则的核心。</li><li>归纳学习依赖于检验数据，因此又称为检验学习。</li><li>归纳推理试图从对象的一部分或整体的特定的观察中获得一个完备且正确的描述。即从部分事实到普遍性规律的结论。</li></ul><h3 id="归纳的基本假设"><a class="markdownIt-Anchor" href="#归纳的基本假设"></a> 归纳的基本假设</h3><p>归纳学习存在一个基本假设：任一假设如果能够在足够大的训练样本集中很好的逼近目标函数，则它也能在测试样本中很好地逼近目标函数。</p><p>该假定是归纳学习的有效性的前提条件。</p><h3 id="决策树模型"><a class="markdownIt-Anchor" href="#决策树模型"></a> 决策树模型</h3><p>分类决策树是一种描述对实例进行分类的树形结构，决策树由结点和有向边构成，结点可分为内部结点和叶结点两种，内部节点表示一个特征或属性，叶节点表示一个分类，通常用圆表示内部结点，用方框表示叶结点。</p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="797984871ce756bea69a8e3cbcde1c5d.png" alt=""></p><p>决策树分类，从根结点开始，对实例某一特征进行测试，根据测试结果将实例分配到其子结点，每一个子结点对应着该特征的一个取值，如此递归直至达到叶结点。</p><p>构造决策树的核心问题是在每一步如何选择适当的属性对样本做拆分。对一个分类问题，从已知类标记的训练样本中学习并构造出决策树是一个自上而下，分而治之的过程。</p><h3 id="决策树的if-then规则"><a class="markdownIt-Anchor" href="#决策树的if-then规则"></a> 决策树的if-then规则</h3><p>由决策树的根结点到叶结点的每一条路径构建一条规则:路径上内部结点的特征对应规则的条件，而叶结点的类对应规则的结论。</p><p>决策树的路径与其对应的if-then规则集合具有一个重要的性质:互斥且完备。这意味着每一个实例都被一条路径或一条规则所覆盖，并且只被一条路径或一条规则所覆盖。</p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="13b78e7dbfe4129c6a69a8dc3fa5bde7.png" alt=""></p><h3 id="决策树与条件概率分布"><a class="markdownIt-Anchor" href="#决策树与条件概率分布"></a> 决策树与条件概率分布</h3><p>决策树表示给定特征条件下类的条件概率分布。条件概率分布定义在特征空间的一个划分上.将特征空间划分为互不相交的单元或区域，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。</p><p>决策树的一条路径对应划分中的一个单元。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。</p><p>决策树学习本质上是从训练数据集中归纳出一组分类规则，能对训练数据进行正确分类的决策树可能有多个，也可能一个也没有。我们需要的是一个与训练数据矛盾较小同时具有较优泛化能力的决策树。即条件概率模型应该不仅对训练数据有很好的拟合，而且对未知数据有很好的预测。</p><h3 id="决策树算法"><a class="markdownIt-Anchor" href="#决策树算法"></a> 决策树算法</h3><p>四个重要算法：CLS、ID3、C4.5、CART。<strong>ID3中使用了信息增益选择特征</strong>，增益大优先选择。<strong>C4.5中，采用信息增益率选择特征</strong>，减少因特征值多导致信息增益大的问题。<strong>CART分类树算法使用基尼系数选择特征</strong>，基尼系数代表了模型的不纯度，<strong>基尼系数越小，不纯度越低，特征越好</strong>。这和信息增益（率）相反。</p><p>算法历程：</p><ul><li>1966年，CLS学习系统</li><li>1979年，简化CLS，得到ID3</li><li>1984年，CART算法</li><li>1986年，基于ID3，创建节点缓冲区，得到ID4</li><li>1988年，基于ID4，优化效率，得到ID5</li><li>1993年，改进ID3，得到C4.5</li></ul><h3 id="决策树cls算法"><a class="markdownIt-Anchor" href="#决策树cls算法"></a> 决策树CLS算法</h3><p>CLS(Concept Learning System)算法是许多决策树学习算法的基础</p><h4 id="基本思想"><a class="markdownIt-Anchor" href="#基本思想"></a> 基本思想</h4><p>CLS的基本思想是从一棵空决策树开始，选择某一分类属性作为测试属性。该测试属性对应决策树中的决策结点。根据该分类属性的值的不同，可将训练样本分成相应的子集。</p><p>若该子集为空，或该子集中的样本属于同一个类，则该子集为叶结点。否则该子集对应于决策树的内部结点，即测试结点，需要选择一个新的分类属性对该子集进行划分，直到所有的子集都为空或者属于同一类。</p><h4 id="算法步骤"><a class="markdownIt-Anchor" href="#算法步骤"></a> 算法步骤</h4><p>1、生成一颗空决策树和一张训练样本属性集；</p><p>2、若训练样本集T中所有的样本都属于同一类,则生成结点T , 并终止学习算法;否则继续；</p><p>3、根据某种策略从训练样本属性表中选择属性A 作为测试属性, 生成测试结点A；</p><blockquote><p>测试属性集的组成以及测试属性的先后对决策树的学习具有举足轻重的影响</p></blockquote><p>4、若A的取值为v1,v2,…,vm, 则根据A的取值的不同,将T划分成m个子集T1,T2,…,Tm；</p><p>5、从训练样本属性表中删除属性A，转至步骤二，对每个子集递归调用CLS。</p><h4 id="算法思考"><a class="markdownIt-Anchor" href="#算法思考"></a> 算法思考</h4><p>实际应用中可以发现，测试属性集的组成以及测试属性的先后对决策树的学习具有举足轻重的影响，不同的特征和不同的选取顺序会生成不同的决策树，因此特征的选择显得尤为重要。</p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="604e59499528d875ca7f33e5af4f7bef.png" alt=""></p><p>不同的特征和不同的选取顺序会生成不同的决策树</p><p>那么，如何选择特征？这一点会在接下来的ID3算法中得到进一步尝试。</p><h3 id="决策树id3算法"><a class="markdownIt-Anchor" href="#决策树id3算法"></a> 决策树ID3算法</h3><p>ID3算法主要针对属性选择问题。是决策树学习方法中最具影响和最为典型的算法。</p><h4 id="基本思想-2"><a class="markdownIt-Anchor" href="#基本思想-2"></a> 基本思想</h4><p>基于CLS的基本思想，ID3算法通过信息增益度选择特征</p><p>当获取信息时，需要将不确定的内容转为确定的内容，因此信息伴着不确定性。从某种程度上讲，小概率事件比大概率事件包含的信息量大，如果某件事情是“百年一见”则肯定比“习以为常”的事件包含的信息量大。那么如何衡量信息量的大小？这就涉及信息论中的概念。</p><h4 id="熵的概念"><a class="markdownIt-Anchor" href="#熵的概念"></a> 熵的概念</h4><p><strong>熵（entropy</strong>）: 信息量大小的度量，也表示随机变量不确定性的度量。</p><p><strong>熵的通俗解释</strong>：事件Ai的信息量<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="eq.png" alt="l(A_i )">可以表示为：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="eq.1.png" alt="l(A_i)=p(A_i)log_2rac{1}{p(A_i)}">,其中<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="eq.2.png" alt="p(A_i)">表示事件Ai发生的概率。</p><p><strong>熵的理论解释</strong>：设X是一个取有限个值的离散随机变量，其概率分布为</p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="eq.3.png" alt="P(X_i=x_i)=p_i,i=1,2,..,n"></p><p>则随机变量X的熵为：</p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="eq.4.png" alt="H=-um_{i=1}^{n}p_ilogp_i"></p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="eq.5.png" alt="0eq Heq logn"></p><p>其中对数以2为底或以e为底，熵的单位分别称为比特(bit)或纳特(nat)。</p><p>熵只依赖于X的分布，与X的取值无关。</p><p><strong>熵越大，对应随机变量的不确定性也越大。</strong></p><p>当X为0，1分布时，<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="eq.6.png" alt="P(X=1)=p">,<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="eq.7.png" alt="P(X=0)=1-p">,<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="eq.8.png" alt="0eq peq 1"></p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="eq.9.png" alt="H=-plog_2p+-(1-p)log_2(1-p)"></p><p>H随p的变化情况可用图表示：</p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="24fa3316dd8078871f344c544bc89ec8.png" alt=""></p><p><strong>条件熵:</strong> <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="eq.10.png" alt="H(Y|X)=um_{i=1}^{n}p_iH(Y|X=x_i)">，表示在己知随机变量X的条件下随机变量Y的不确定性，定义为X给定条件下Y的条件概率分布的熵对X的数学期.</p><p>当熵和条件熵中的概率由数据估计(特别是极大似然估计)得到时，所对应的熵与条件熵分别称为经验熵和经验条件熵.</p><h4 id="信息增益"><a class="markdownIt-Anchor" href="#信息增益"></a> 信息增益</h4><p>信息增益(Information gain)：特征A对训练数据集D的信息增益g(D,A), 定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="eq.11.png" alt="g(D,A)=H(D)-H(D|A)"></p><p>信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度，一般来说<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="eq.12.png" alt="H(Y)-H(Y|X)">称为互信息，决策树学习中的信息增益等价于训练数据集中类与特征的互信息。</p><h4 id="信息增益的算法"><a class="markdownIt-Anchor" href="#信息增益的算法"></a> 信息增益的算法</h4><p>输入：训练数据集D和特征A；</p><p>1、计算数据集D得经验熵：</p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="eq.13.png" alt="H(D)=-um_{k=1}^{K}rac{|C_k|}{|D|}log_2rac{|C_k|}{|D|}"></p><p>2、计算特征A对数据集D的经验条件熵H(D|A)</p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="eq.14.png" alt="H(D|A)=um_{I=1}{n}\frac{|D_i|}{|D|}H(D_i)=-\sum_{I=1}{n}rac{|D_i|}{|D|}um_{k=1}^{K}rac{|D_{ik}|}{|D|}log_2rac{|D_{ik}|}{|D|}"></p><p>3、计算信息增益</p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="eq.15.png" alt="g(D,A)=H(D)-H(D|A)"></p><p>输出：特征A对训练数据集D的信息增益<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="eq.11.png" alt="g(D,A)">。</p><blockquote><ul><li>|Ck |为属于类Ck的样本个数</li><li>特征A有n个不同的 取值{a1,a2…an}根据特征A的取值 将D划分为n个子集D1…Dn</li><li>子集Di中属于类Ck的样本集合为Dik</li></ul></blockquote><h4 id="id3算法"><a class="markdownIt-Anchor" href="#id3算法"></a> ID3算法</h4><p>1、从根节点开始，计算所有可能的特征的信息增益，选择信息增益最大的特征作为节点的划分特征；</p><p>2、由该特征的不同取值建立子节点；</p><p>3、再对子节点递归1-2步，构建决策树；</p><p>4、直到没有特征可以选择或类别完全相同为止，得到最终的决策树。</p><h4 id="算法思考-2"><a class="markdownIt-Anchor" href="#算法思考-2"></a> 算法思考</h4><p>ID3算法以信息熵为度量，用于决策树节点的属性选择，每次优先选取信息量最多的属性，亦即能使熵值变为最小的属性，以构造一颗熵值下降最快的决策树，到叶子节点处的熵值为0。此时，每个叶子节点对应的实例集中的实例属于同一类。</p><h3 id="决策树c45算法"><a class="markdownIt-Anchor" href="#决策树c45算法"></a> 决策树C4.5算法</h3><p><strong>ID3中使用了信息增益选择特征</strong>，增益大优先选择。<strong>C4.5中，采用信息增益率选择特征</strong>，减少因特征值多导致信息增益大的问题。以信息增益作为划分训练数据集的特征，易偏向于选择取值较多的特征，考虑信息增益比可以矫正这一问题。</p><h4 id="信息增益比"><a class="markdownIt-Anchor" href="#信息增益比"></a> 信息增益比</h4><p>特征A对训练数据集D的信息增益比定义为信息增益与训练数据集D关于特征A的值的熵之比</p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="eq.16.png" alt="g_R(D,A)=rac{g(D,A)}{H_A(D)}"></p><p>其中<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="eq.17.png" alt="H_A(D)=-um_{i=1}^{n}rac{|D_i|}{|D|}log_2rac{|D_i|}{|D|}">，n是特征A的取值个数</p><h3 id="决策树cart算法"><a class="markdownIt-Anchor" href="#决策树cart算法"></a> 决策树CART算法</h3><p>ID3和C4.5算法，生成的决策树是多叉树，只能处理分类不能处理回归。而CART（classification and regression tree）分类回归树算法，既可用于分类也可用于回归。 分类树的输出是样本的类别， 回归树的输出是一个实数。</p><h4 id="算法组成"><a class="markdownIt-Anchor" href="#算法组成"></a> 算法组成</h4><ul><li>决策树生成</li><li>决策树剪枝</li></ul><h4 id="catr树"><a class="markdownIt-Anchor" href="#catr树"></a> CATR树</h4><ul><li>目标变量是类别的——分类树：Gini指标、Towing、order Towing</li><li>目标变量是连续的——回归树：最小平方残差、最小绝对残差</li></ul><h4 id="基尼系数"><a class="markdownIt-Anchor" href="#基尼系数"></a> 基尼系数</h4><p>数据集D的纯度可用基尼值来度量</p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="eq.18.png" alt="Gini(D)=um_{i=1}{n}p(x_i)(1-p(x_i))=1-\sum_{i=1}{n}p(x_i)^{2}"></p><p>其中， p(xi) 是分类 xi 出现的概率，n是分类的数目。Gini(D)反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率。因此，<strong>Gini(D)越小，则数据集D的纯度越高</strong>。</p><p>对于样本D，样本容量为|D|，根据特征A 是否取某一可能值a，把样本D分成两部分D1和D2 ，所以CART分类树算法建立起来的是二叉树，而不是多叉树。</p><p>在属性A的条件下，样本D的基尼系数定义为</p><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="eq.19.png" alt="Giniindex(D|A=a)=rac{|D_1|}{|D|}Gini(D_1)+rac{|D_2|}{|D|}Gini(D_2)"></p><h3 id="决策树的优点"><a class="markdownIt-Anchor" href="#决策树的优点"></a> 决策树的优点</h3><p>1、决策推理过程可以表示成If-Then的形式；</p><p>2、推理过程完全依赖于属性变量的取值特点；</p><p>3、可忽略对目标变量无贡献的属性变量。</p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://www.formeasy.cc">formeasy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://www.formeasy.cc/2025/07/10/algo/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%91%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%88Decision%20Tree%EF%BC%89/">http://www.formeasy.cc/2025/07/10/algo/【机器学习】决策树（Decision Tree）/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="http://www.formeasy.cc" target="_blank">易锦风的博客</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/algo/">algo</a></div><div class="post-share"><div class="social-share" data-image="/2025/07/10/algo/%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%91%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%88Decision%20Tree%EF%BC%89/797984871ce756bea69a8e3cbcde1c5d.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/07/11/C/vector%E7%9A%84%E8%AF%A6%E7%BB%86%E8%AE%B2%E8%A7%A3/" title="vector的详细讲解"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/07/11/C/vector%E7%9A%84%E8%AF%A6%E7%BB%86%E8%AE%B2%E8%A7%A3/53d1d2e536204e609dc694bcb054ce9c.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">vector的详细讲解</div></div><div class="info-2"><div class="info-item-1">1.vector的介绍及使用 1.1 vector的介绍 1. vector 是表示可变大小数组的序列容器。 2. 就像数组一样， vector 也采用的连续存储空间来存储元素。也就是意味着可以采用下标对 vector 的元素 进行访问，和数组一样高效。但是又不像数组，它的大小是可以动态改变的，而且它的大小会被容器自 动处理。 3. 本质讲， vector 使用动态分配数组来存储它的元素。当新元素插入时候，这个数组需要被重新分配大小 为了增加存储空间。其做法是，分配一个新的数组，然后将全部元素移到这个数组。就时间而言，这是 一个相对代价高的任务，因为每当一个新的元素加入到容器的时候，vector 并不会每次都重新分配大 小。 4. vector 分配空间策略： vector 会分配一些额外的空间以适应可能的增长，因为存储空间比实际需要的存 储空间更大。不同的库采用不同的策略权衡空间的使用和重新分配。但是无论如何，重新分配都应该是 对数增长的间隔大小，以至于在末尾插入一个元素的时候是在常数时间的复杂度完成的。 5. 因此， vector...</div></div></div></a><a class="pagination-related" href="/2025/07/10/C/C++%E5%8A%A8%E6%80%81%E5%88%86%E9%85%8D%E5%86%85%E5%AD%98%E7%9F%A5%E8%AF%86%E7%82%B9%EF%BC%81/" title="C++动态分配内存知识点！"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">C++动态分配内存知识点！</div></div><div class="info-2"><div class="info-item-1">1.动态分配内存的思想 动态分配内存是指在程序运行时根据需要动态地分配内存空间。这相对于静态分配内存来说，静态分配内存是在编译时固定地分配内存空间，而动态分配内存可以在程序运行期间根据实际需求进行内存的申请和释放，以提高内存的利用率和灵活性。 2.动态分配内存的概念 动态分配内存的概念包括以下几个方面： 2.1内存分配函数 动态分配内存需要使用内存分配函数，如C语言中的malloc()、calloc()、realloc()等，这些函数可以根据需要在运行时动态地分配一块连续的内存空间。 2.2动态内存的申请和释放 使用内存分配函数可以申请一块指定大小的内存空间，申请的内存空间可以在程序运行期间使用。使用完毕后，可以使用释放函数将内存空间释放，以便其他程序继续使用。 2.3内存碎片问题 动态分配内存可能会导致内存碎片问题。当频繁地进行内存分配和释放操作时，可能会在内存中留下一些未被使用的小块内存，这些小块内存无法被再次利用，导致内存的浪费。为了解决内存碎片问题，可以使用内存管理算法来进行内存的分配和释放操作。 ...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/06/16/algo/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E7%AE%97%E6%B3%95%EF%BC%88C++%EF%BC%89/" title="蒙特卡洛算法（C++）"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/06/16/algo/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E7%AE%97%E6%B3%95%EF%BC%88C++%EF%BC%89/b6ed8fe16c0b4534af76a8e0eb927efd.jpeg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-16</div><div class="info-item-2">蒙特卡洛算法（C++）</div></div><div class="info-2"><div class="info-item-1">蒙特卡洛算法是一个基于几何概率模型的近似估计真实值的方法，可以近似估计出圆周率π和一些被积函数比较复杂不容易求出积分的积分值。 近似估计出圆周率π举例： 假设在正方形内投掷随机点数量为N(N∈N*)，则按几何概率，当N很大时，落在圆中数量为n(n∈N*), 而N与n的比值等价于两者的面积比，即： πr²/4r²= n/N =&gt;  π = 4n/N C++代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#include &lt;iostream&gt;#include &lt;random&gt;#include &lt;cmath&gt; const double r = 2; double Random(int min, int max); class location &#123;public:	double x, y; location(double x, double y)...</div></div></div></a><a class="pagination-related" href="/2025/06/16/algo/%E8%92%99%E7%89%B9%E7%BD%97%E5%8D%A1%CF%80%E7%AE%97%E6%B3%95%EF%BC%88C++%E8%AF%AD%E8%A8%80%E6%8F%8F%E8%BF%B0%EF%BC%89/" title="蒙特罗卡π算法（C++语言描述）"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/06/16/algo/%E8%92%99%E7%89%B9%E7%BD%97%E5%8D%A1%CF%80%E7%AE%97%E6%B3%95%EF%BC%88C++%E8%AF%AD%E8%A8%80%E6%8F%8F%E8%BF%B0%EF%BC%89/watermark2textaHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXN5emI=font5a6L5L2Tfontsize400fillI0JBQkFCMA==dissolve70gravity.png" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-16</div><div class="info-item-2">蒙特罗卡π算法（C++语言描述）</div></div><div class="info-2"><div class="info-item-1">圆的面积计算公式为：S=π*r*r 将圆放到一个直角坐标系中，如图黄色部分的面积是S/4=(π*r*r)/4;如果我们将取一个单位圆，则S/4=π/4. 因为是单位圆，半径为1，所以图中红色正方形的面积为1。 那么如果向正方形内均匀的撒点，那么落入阴影部分的点数与全部的点数之比应该是： S阴影/S正方形=π/4.==============》π=4*S阴影/S正方形 根据概率统计的规律，只要撒的点足够多，那么将得到近似的结果。 使用蒙特卡罗算法计算圆周率有如下两个关键点： 均匀撒点：通过rand函数残生[0,1]之间随即的坐标值[x,y] 区域判断：图中黄色部分的特点是距离坐标原点的距离小于等于1，这样，可以通过计算判断x2+y2&lt;=1来实现。 C++语言代码： 123456789101112131415161718192021222324252627282930313233#include&lt;iostream&gt;#include&lt;ctime&gt;#define s_rand() double(1.0*rand()/RAND_MAX)using...</div></div></div></a></div></div><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/butterfly-icon.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">formeasy</div><div class="author-info-description">专注互联网和软件技术</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">242</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">39</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/formeasy"><i class="fab fa-github"></i><span>关注我</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/formeasy" target="_blank" title="Github"><i class="fab fa-github" style="color:#24292e"></i></a><a class="social-icon" href="mailto:formeasy@163.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color:#4a7dbe"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">用学习，面对遭遇的变化；用斗志，面对每天的挫折；用坚持，面对失去的动力!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%95%E5%85%A5"><span class="toc-number">1.</span> <span class="toc-text">引入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-number">2.</span> <span class="toc-text">关于分类问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E6%AD%A5%E9%AA%A4"><span class="toc-number">3.</span> <span class="toc-text">分类问题的步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text"></span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E%E5%BD%92%E7%BA%B3%E7%AE%97%E6%B3%95"><span class="toc-number">5.</span> <span class="toc-text">关于归纳算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%92%E7%BA%B3%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%81%87%E8%AE%BE"><span class="toc-number">6.</span> <span class="toc-text">归纳的基本假设</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E6%A8%A1%E5%9E%8B"><span class="toc-number">7.</span> <span class="toc-text">决策树模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84if-then%E8%A7%84%E5%88%99"><span class="toc-number">8.</span> <span class="toc-text">决策树的if-then规则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E4%B8%8E%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83"><span class="toc-number">9.</span> <span class="toc-text">决策树与条件概率分布</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95"><span class="toc-number">10.</span> <span class="toc-text">决策树算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91cls%E7%AE%97%E6%B3%95"><span class="toc-number">11.</span> <span class="toc-text">决策树CLS算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3"><span class="toc-number">11.1.</span> <span class="toc-text">基本思想</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4"><span class="toc-number">11.2.</span> <span class="toc-text">算法步骤</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%80%9D%E8%80%83"><span class="toc-number">11.3.</span> <span class="toc-text">算法思考</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91id3%E7%AE%97%E6%B3%95"><span class="toc-number">12.</span> <span class="toc-text">决策树ID3算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3-2"><span class="toc-number">12.1.</span> <span class="toc-text">基本思想</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%86%B5%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-number">12.2.</span> <span class="toc-text">熵的概念</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A"><span class="toc-number">12.3.</span> <span class="toc-text">信息增益</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E7%9A%84%E7%AE%97%E6%B3%95"><span class="toc-number">12.4.</span> <span class="toc-text">信息增益的算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#id3%E7%AE%97%E6%B3%95"><span class="toc-number">12.5.</span> <span class="toc-text">ID3算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%80%9D%E8%80%83-2"><span class="toc-number">12.6.</span> <span class="toc-text">算法思考</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91c45%E7%AE%97%E6%B3%95"><span class="toc-number">13.</span> <span class="toc-text">决策树C4.5算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E6%AF%94"><span class="toc-number">13.1.</span> <span class="toc-text">信息增益比</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91cart%E7%AE%97%E6%B3%95"><span class="toc-number">14.</span> <span class="toc-text">决策树CART算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E7%BB%84%E6%88%90"><span class="toc-number">14.1.</span> <span class="toc-text">算法组成</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#catr%E6%A0%91"><span class="toc-number">14.2.</span> <span class="toc-text">CATR树</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E5%B0%BC%E7%B3%BB%E6%95%B0"><span class="toc-number">14.3.</span> <span class="toc-text">基尼系数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E4%BC%98%E7%82%B9"><span class="toc-number">15.</span> <span class="toc-text">决策树的优点</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/12/21/UE/UE5%20-%20%E9%99%84%E5%8A%A0%E8%8A%82%E7%82%B9%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3%EF%BC%88%E5%B0%86%E7%BB%84%E4%BB%B6%E6%88%96%E8%80%85Actor%E9%99%84%E5%8A%A0%E5%88%B0%E5%8F%A6%E4%B8%80%E4%B8%AA%E7%BB%84%E4%BB%B6%E4%B8%8A%EF%BC%89/" title="UE5 - 附加节点使用详解（将组件或者Actor附加到另一个组件上）"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/12/21/UE/UE5%20-%20%E9%99%84%E5%8A%A0%E8%8A%82%E7%82%B9%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3%EF%BC%88%E5%B0%86%E7%BB%84%E4%BB%B6%E6%88%96%E8%80%85Actor%E9%99%84%E5%8A%A0%E5%88%B0%E5%8F%A6%E4%B8%80%E4%B8%AA%E7%BB%84%E4%BB%B6%E4%B8%8A%EF%BC%89/2025060520004171900.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="UE5 - 附加节点使用详解（将组件或者Actor附加到另一个组件上）"></a><div class="content"><a class="title" href="/2025/12/21/UE/UE5%20-%20%E9%99%84%E5%8A%A0%E8%8A%82%E7%82%B9%E4%BD%BF%E7%94%A8%E8%AF%A6%E8%A7%A3%EF%BC%88%E5%B0%86%E7%BB%84%E4%BB%B6%E6%88%96%E8%80%85Actor%E9%99%84%E5%8A%A0%E5%88%B0%E5%8F%A6%E4%B8%80%E4%B8%AA%E7%BB%84%E4%BB%B6%E4%B8%8A%EF%BC%89/" title="UE5 - 附加节点使用详解（将组件或者Actor附加到另一个组件上）">UE5 - 附加节点使用详解（将组件或者Actor附加到另一个组件上）</a><time datetime="2025-12-21T12:52:42.000Z" title="发表于 2025-12-21 20:52:42">2025-12-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/12/UE/%E3%80%90UE5%E3%80%91%E7%BD%91%E7%BB%9C%E5%90%8C%E6%AD%A5%EF%BC%8C%E4%B8%8D%E5%90%8C%E6%83%85%E5%86%B5%E7%9A%84%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0/" title="【UE5】网络同步，不同情况的简单实现"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/12/12/UE/%E3%80%90UE5%E3%80%91%E7%BD%91%E7%BB%9C%E5%90%8C%E6%AD%A5%EF%BC%8C%E4%B8%8D%E5%90%8C%E6%83%85%E5%86%B5%E7%9A%84%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0/d9825c2ae6dfdfb33572628841ba985f26596343.png@968w_804h.webp" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="【UE5】网络同步，不同情况的简单实现"></a><div class="content"><a class="title" href="/2025/12/12/UE/%E3%80%90UE5%E3%80%91%E7%BD%91%E7%BB%9C%E5%90%8C%E6%AD%A5%EF%BC%8C%E4%B8%8D%E5%90%8C%E6%83%85%E5%86%B5%E7%9A%84%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0/" title="【UE5】网络同步，不同情况的简单实现">【UE5】网络同步，不同情况的简单实现</a><time datetime="2025-12-12T05:32:54.000Z" title="发表于 2025-12-12 13:32:54">2025-12-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/12/UE/UE5%E6%97%B6%E9%97%B4%E8%BD%B4%E8%8A%82%E7%82%B9%E5%8F%8A%E5%85%B6%E8%AE%BE%E7%BD%AE/" title="UE5时间轴节点及其设置"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/cover.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="UE5时间轴节点及其设置"></a><div class="content"><a class="title" href="/2025/12/12/UE/UE5%E6%97%B6%E9%97%B4%E8%BD%B4%E8%8A%82%E7%82%B9%E5%8F%8A%E5%85%B6%E8%AE%BE%E7%BD%AE/" title="UE5时间轴节点及其设置">UE5时间轴节点及其设置</a><time datetime="2025-12-12T05:31:05.000Z" title="发表于 2025-12-12 13:31:05">2025-12-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/11/20/UE/%E3%80%90UE5%E3%80%91%E6%91%84%E5%83%8F%E6%9C%BA%E6%99%83%E5%8A%A8/" title="【UE5】摄像机晃动"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/11/20/UE/%E3%80%90UE5%E3%80%91%E6%91%84%E5%83%8F%E6%9C%BA%E6%99%83%E5%8A%A8/82aaa08690314d64bf6b173d848ea095.gif" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="【UE5】摄像机晃动"></a><div class="content"><a class="title" href="/2025/11/20/UE/%E3%80%90UE5%E3%80%91%E6%91%84%E5%83%8F%E6%9C%BA%E6%99%83%E5%8A%A8/" title="【UE5】摄像机晃动">【UE5】摄像机晃动</a><time datetime="2025-11-20T01:41:51.000Z" title="发表于 2025-11-20 09:41:51">2025-11-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/11/20/UE/%E3%80%90Cesium%20for%20Unreal%E3%80%91%E6%B0%B4%E6%95%88%E6%9E%9C%E5%AE%9E%E7%8E%B0/" title="【Cesium for Unreal】水效果实现"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/2025/11/20/UE/%E3%80%90Cesium%20for%20Unreal%E3%80%91%E6%B0%B4%E6%95%88%E6%9E%9C%E5%AE%9E%E7%8E%B0/64130adb59dd4d2c9188ffb8d5c2d3e2.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="【Cesium for Unreal】水效果实现"></a><div class="content"><a class="title" href="/2025/11/20/UE/%E3%80%90Cesium%20for%20Unreal%E3%80%91%E6%B0%B4%E6%95%88%E6%9E%9C%E5%AE%9E%E7%8E%B0/" title="【Cesium for Unreal】水效果实现">【Cesium for Unreal】水效果实现</a><time datetime="2025-11-20T01:25:49.000Z" title="发表于 2025-11-20 09:25:49">2025-11-20</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By formeasy</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="前往评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>(async()=>{window.katex_js_css||(window.katex_js_css=!0,await btf.getCSS("https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"),await btf.getScript("https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js")),document.querySelectorAll("#article-container .katex").forEach(t=>t.classList.add("katex-show"))})()</script><script>(()=>{var e=()=>{var e;0!==(e=document.querySelectorAll("pre > code.mermaid")).length&&e.forEach(e=>{var t=document.createElement("pre"),n=(t.className="mermaid-src",t.hidden=!0,t.textContent=e.textContent,document.createElement("div"));n.className="mermaid-wrap",n.appendChild(t),e.parentNode.replaceWith(n)});const t=document.querySelectorAll("#article-container .mermaid-wrap");0!==t.length&&(e=()=>{{var e=t;window.loadMermaid=!0;const a="dark"===document.documentElement.getAttribute("data-theme")?"dark":"default";e.forEach((e,t)=>{const n=e.firstElementChild;e=`%%{init:{ 'theme':'${a}'}}%%
`+n.textContent,t=mermaid.render("mermaid-"+t,e);const d=e=>{n.insertAdjacentHTML("afterend",e)};"string"==typeof t?d(t):t.then(({svg:e})=>d(e))})}},btf.addGlobalFn("themeChange",e,"mermaid"),window.loadMermaid?e():btf.getScript("https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js").then(e))};btf.addGlobalFn("encrypt",e,"mermaid"),window.pjax?e():document.addEventListener("DOMContentLoaded",e)})()</script><script>(()=>{const a=GLOBAL_CONFIG_SITE.isShuoshuo,s=null,n=t=>"dark"===t?"dark":"light";var t=(t=document,e)=>{e=a?{"data-mapping":"specific","data-term":e}:{"data-mapping":(s,"pathname")},e=(t=>{const a=document.createElement("script");return Object.entries(t).forEach(([t,e])=>{a.setAttribute(t,e)}),a})({src:"https://giscus.app/client.js","data-repo":"formeasy/blog","data-repo-id":"R_kgDOJ2sA5A","data-category-id":"DIC_kwDOJ2sA5M4CktKh","data-theme":n(document.documentElement.getAttribute("data-theme")),"data-reactions-enabled":"1",crossorigin:"anonymous",async:!0,...s,...e});t.querySelector("#giscus-wrap").appendChild(e),a&&(window.shuoshuoComment.destroyGiscus=()=>{t.children.length&&(t.innerHTML="",t.classList.add("no-comment"))})};btf.addGlobalFn("themeChange",t=>{var e=document.querySelector("#giscus-wrap iframe");e&&(t={giscus:{setConfig:{theme:n(t)}}},e.contentWindow.postMessage(t,"https://giscus.app"))},"giscus"),a?window.shuoshuoComment={loadComment:t}:t()})()</script></div><script defer id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zindex="-1" mobile="false" data-click="true"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="text-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span> 数据加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="请输入搜索关键字" type="text"></div></div><hr><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>